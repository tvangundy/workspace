# https://taskfile.dev

version: '3'

tasks:
  create:
    silent: true
    desc: Create an Ubuntu virtual machine instance using Terraform. Use --windsor-up to also run aqua install, windsor init local, and windsor up after VM creation.
    cmds:
      - task: create:validate
      - task: generate-tfvars
      - task: terraform:init
      - task: terraform:apply
      - |
        set -euo pipefail
        # Parse CLI arguments to get instance name and flags for setup tasks
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        
        # Use defaults from environment variables
        INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
        INSTANCE_NAME_ARG="${INSTANCE_NAME_ARG:-vm}"
        RUN_WINDSOR_UP=false
        
        # Override with CLI arguments if provided
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          # Parse arguments
          while [ $# -gt 0 ]; do
            case "$1" in
              --name)
                INSTANCE_NAME_ARG="${2}"
                shift 2
                ;;
              --windsor-up)
                RUN_WINDSOR_UP=true
                shift
                ;;
              *)
                shift
                ;;
            esac
          done
        fi
        
        # Setup developer environment for remote deployments
        if [ "{{.INCUS_REMOTE_NAME}}" != "local" ]; then
          task vm:create:setup-env -- ${INSTANCE_NAME_ARG}
        fi
        
        # Get instance information (with error handling to prevent task failure)
        set +e  # Temporarily disable exit on error for info gathering
        REMOTE_NAME="{{.INCUS_REMOTE_NAME}}"
        CURRENT_USER=$(whoami)
        VM_IMAGE="{{.VM_IMAGE}}"
        VM_IMAGE="${VM_IMAGE:-ubuntu/24.04}"
        VM_MEMORY="{{.VM_MEMORY}}"
        VM_CPU="{{.VM_CPU}}"
        VM_DISK_SIZE="{{.VM_DISK_SIZE}}"
        
        # Wait a moment for VM to be fully ready
        sleep 2
        
        # Get IP address
        VM_IP=""
        if [ "${REMOTE_NAME}" != "local" ]; then
          # Try multiple methods to get IP
          VM_IP=$(incus list "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
            grep -o '"4":{[^}]*"addresses":[^}]*}' | \
            grep -o '"address":"[^"]*"' | head -1 | cut -d'"' -f4 2>/dev/null || echo "")
          
          # Fallback: try to get IP from inside the VM
          if [ -z "${VM_IP}" ]; then
            VM_IP=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- \
              ip -4 addr show 2>/dev/null | grep "inet " | grep -v "127.0.0.1" | \
              head -1 | awk '{print $2}' | cut -d'/' -f1 2>/dev/null || echo "")
          fi
          
          # Another fallback: try hostname -I
          if [ -z "${VM_IP}" ]; then
            VM_IP=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- \
              hostname -I 2>/dev/null | awk '{print $1}' 2>/dev/null || echo "")
          fi
        fi
        
        # Get status
        VM_STATUS="unknown"
        if command -v jq > /dev/null 2>&1; then
          VM_STATUS=$(incus list "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
            jq -r '.[0].status // "unknown"' 2>/dev/null || echo "unknown")
        else
          VM_STATUS=$(incus list "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" --format csv -c ns 2>/dev/null | \
            grep "^${INSTANCE_NAME_ARG}," | cut -d',' -f2 2>/dev/null || echo "unknown")
        fi
        set -e  # Re-enable exit on error
        
        echo ""
        echo "═══════════════════════════════════════════════════════════════"
        echo "✅ Instance '${INSTANCE_NAME_ARG}' created successfully"
        echo "═══════════════════════════════════════════════════════════════"
        echo ""
        
        # Windsor Setup and Up (if --windsor-up flag is set)
        if [ "${RUN_WINDSOR_UP}" = "true" ] && [ "${REMOTE_NAME}" != "local" ]; then
        echo ""
          echo "═══════════════════════════════════════════════════════════════"
          echo "Windsor Setup and Up"
          echo "═══════════════════════════════════════════════════════════════"
          echo ""
          
          # Get workspace path
          if [ "{{.VM_INIT_WORKSPACE}}" = "true" ]; then
            WORKSPACE_NAME=$(basename "{{.WINDSOR_PROJECT_ROOT}}")
            WORKSPACE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
          else
            # Workspace might not be initialized, but we can still try
            WORKSPACE_NAME=$(basename "{{.WINDSOR_PROJECT_ROOT}}")
            WORKSPACE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
          fi
          
          # Check if workspace exists
          if ! incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- test -d "${WORKSPACE_PATH}" 2>/dev/null; then
            echo "⚠️  Warning: Workspace directory does not exist: ${WORKSPACE_PATH}"
            echo "   Skipping Windsor setup. Initialize workspace first or use --windsor-up with workspace initialization."
            echo ""
            echo "   To initialize workspace:"
            echo "     task vm:init-workspace -- ${INSTANCE_NAME_ARG}"
            echo ""
            echo "   Or recreate VM with workspace initialization:"
            echo "     task vm:create -- ${INSTANCE_NAME_ARG}  # (with VM_INIT_WORKSPACE=true)"
          else
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Step 1: Install Aqua Packages"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "  Running: aqua install"
            echo ""
            
            set +e  # Temporarily disable exit on error
            # Use a script file to avoid quoting issues
            AQUA_OUTPUT=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- bash -c "
              printf '%s\n' \
                '#!/bin/bash' \
                \"cd ${WORKSPACE_PATH}\" \
                'if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then' \
                '  eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" 2>/dev/null || true' \
                'fi' \
                'export AQUA_ROOT_DIR=\"\${AQUA_ROOT_DIR:-\${XDG_DATA_HOME:-\$HOME/.local/share}/aquaproj-aqua}\"' \
                'export PATH=\"\${AQUA_ROOT_DIR}/bin:\${PATH}\"' \
                'source ~/.bashrc 2>/dev/null || true' \
                'aqua install 2>&1' > /tmp/aqua-install.sh
              chmod +x /tmp/aqua-install.sh
              sudo -u ${CURRENT_USER} -i bash /tmp/aqua-install.sh
            " 2>&1)
            AQUA_EXIT=$?
            set -e  # Re-enable exit on error
            
            if [ ${AQUA_EXIT} -eq 0 ]; then
              echo "✅ aqua install completed successfully"
              echo ""
              if [ -n "${AQUA_OUTPUT}" ]; then
                echo "  Output:"
                echo "${AQUA_OUTPUT}" | sed 's/^/    /'
              else
                echo "  (No output)"
              fi
            else
              echo "❌ aqua install failed (exit code: ${AQUA_EXIT})"
              echo ""
              echo "  Output:"
              if [ -n "${AQUA_OUTPUT}" ]; then
                echo "${AQUA_OUTPUT}" | tail -30 | sed 's/^/    /'
              else
                echo "    (No output captured)"
              fi
            fi
            
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Step 2: Initialize Windsor Context"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "  Running: windsor init local"
            echo ""
            
            set +e  # Temporarily disable exit on error
            # Use a script file to avoid quoting issues
            WINDSOR_INIT_OUTPUT=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- bash -c "
              printf '%s\n' \
                '#!/bin/bash' \
                \"cd ${WORKSPACE_PATH}\" \
                'if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then' \
                '  eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" 2>/dev/null || true' \
                'fi' \
                'export AQUA_ROOT_DIR=\"\${AQUA_ROOT_DIR:-\${XDG_DATA_HOME:-\$HOME/.local/share}/aquaproj-aqua}\"' \
                'export PATH=\"\${AQUA_ROOT_DIR}/bin:\${PATH}\"' \
                'source ~/.bashrc 2>/dev/null || true' \
                'windsor init local 2>&1' > /tmp/windsor-init.sh
              chmod +x /tmp/windsor-init.sh
              sudo -u ${CURRENT_USER} -i bash /tmp/windsor-init.sh
            " 2>&1)
            WINDSOR_INIT_EXIT=$?
            set -e  # Re-enable exit on error
            
            if [ ${WINDSOR_INIT_EXIT} -eq 0 ]; then
              echo "✅ windsor init local completed successfully"
              echo ""
              if [ -n "${WINDSOR_INIT_OUTPUT}" ]; then
                echo "  Output:"
                echo "${WINDSOR_INIT_OUTPUT}" | sed 's/^/    /'
              else
                echo "  (No output)"
              fi
            else
              echo "❌ windsor init local failed (exit code: ${WINDSOR_INIT_EXIT})"
              echo ""
              echo "  Output:"
              if [ -n "${WINDSOR_INIT_OUTPUT}" ]; then
                echo "${WINDSOR_INIT_OUTPUT}" | tail -30 | sed 's/^/    /'
              else
                echo "    (No output captured)"
              fi
            fi
            
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Step 3: Start Windsor Environment"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "  Running: windsor up (this may take several minutes)..."
            echo ""
            
            set +e  # Temporarily disable exit on error
            # Use a script file to avoid quoting issues
            # Use 'script' command to create a pseudo-TTY for windsor up
            WINDSOR_UP_OUTPUT=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- bash -c "
              printf '%s\n' \
                '#!/bin/bash' \
                \"cd ${WORKSPACE_PATH}\" \
                'if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then' \
                '  eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" 2>/dev/null || true' \
                'fi' \
                'export AQUA_ROOT_DIR=\"\${AQUA_ROOT_DIR:-\${XDG_DATA_HOME:-\$HOME/.local/share}/aquaproj-aqua}\"' \
                'export PATH=\"\${AQUA_ROOT_DIR}/bin:\${PATH}\"' \
                'source ~/.bashrc 2>/dev/null || true' \
                'export WINDSOR_NON_INTERACTIVE=1' \
                'if command -v script >/dev/null 2>&1; then' \
                '  script -qec \"windsor up 2>&1\" /dev/null' \
                'else' \
                '  windsor up 2>&1' \
                'fi' > /tmp/windsor-up.sh
              chmod +x /tmp/windsor-up.sh
              sudo -u ${CURRENT_USER} -i bash /tmp/windsor-up.sh < /dev/null
            " 2>&1)
            WINDSOR_UP_EXIT=$?
            set -e  # Re-enable exit on error
            
            if [ ${WINDSOR_UP_EXIT} -eq 0 ]; then
              echo "✅ windsor up completed successfully"
              echo ""
              if [ -n "${WINDSOR_UP_OUTPUT}" ]; then
                echo "  Output (last 50 lines):"
                echo "${WINDSOR_UP_OUTPUT}" | tail -50 | sed 's/^/    /'
              else
                echo "  (No output)"
              fi
              
              # Try to get pod status
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "Step 4: Verify Pod Status"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              
              set +e  # Temporarily disable exit on error
              sleep 3  # Wait a moment for pods to stabilize
              
              # Use a script file to avoid quoting issues
              POD_STATUS_OUTPUT=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- bash -c "
                printf '%s\n' \
                  '#!/bin/bash' \
                  \"cd ${WORKSPACE_PATH}\" \
                  'if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then' \
                  '  eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" 2>/dev/null || true' \
                  'fi' \
                  'export AQUA_ROOT_DIR=\"\${AQUA_ROOT_DIR:-\${XDG_DATA_HOME:-\$HOME/.local/share}/aquaproj-aqua}\"' \
                  'export PATH=\"\${AQUA_ROOT_DIR}/bin:\${PATH}\"' \
                  'source ~/.bashrc 2>/dev/null || true' \
                  \"if [ -f ${WORKSPACE_PATH}/contexts/local/.kube/config ]; then\" \
                  \"  export KUBECONFIG=${WORKSPACE_PATH}/contexts/local/.kube/config\" \
                  'fi' \
                  'kubectl get pods --all-namespaces 2>&1 || echo \"kubectl not available or kubeconfig not found\"' > /tmp/kubectl-pods.sh
                chmod +x /tmp/kubectl-pods.sh
                sudo -u ${CURRENT_USER} -i bash /tmp/kubectl-pods.sh
              " 2>&1)
              POD_STATUS_EXIT=$?
              set -e  # Re-enable exit on error
              
              if [ ${POD_STATUS_EXIT} -eq 0 ] && ! echo "${POD_STATUS_OUTPUT}" | grep -q "kubectl not available"; then
                echo "  Pod Status:"
                echo "${POD_STATUS_OUTPUT}" | head -30 | sed 's/^/    /'
                
                # Count pods by status
                RUNNING_PODS=$(echo "${POD_STATUS_OUTPUT}" | grep -c "Running" || echo "0")
                PENDING_PODS=$(echo "${POD_STATUS_OUTPUT}" | grep -c "Pending" || echo "0")
                FAILED_PODS=$(echo "${POD_STATUS_OUTPUT}" | grep -cE "(Error|CrashLoopBackOff|ImagePullBackOff)" || echo "0")
                
                echo ""
                echo "  Pod Summary:"
                echo "    Running: ${RUNNING_PODS}"
                echo "    Pending: ${PENDING_PODS}"
                echo "    Failed:  ${FAILED_PODS}"
                
                if [ ${FAILED_PODS} -gt 0 ]; then
                  echo ""
                  echo "  ⚠️  Warning: Some pods are in failed state:"
                  echo "${POD_STATUS_OUTPUT}" | grep -E "(Error|CrashLoopBackOff|ImagePullBackOff)" | head -10 | sed 's/^/    /'
                fi
              else
                echo "  ⚠️  Could not verify pod status (kubectl may not be configured yet)"
                if [ -n "${POD_STATUS_OUTPUT}" ]; then
                  echo "${POD_STATUS_OUTPUT}" | tail -5 | sed 's/^/    /'
                fi
              fi
            else
              echo "❌ windsor up failed (exit code: ${WINDSOR_UP_EXIT})"
              echo ""
              echo "  Output (last 50 lines):"
              if [ -n "${WINDSOR_UP_OUTPUT}" ]; then
                echo "${WINDSOR_UP_OUTPUT}" | tail -50 | sed 's/^/    /'
              else
                echo "    (No output captured)"
              fi
            fi
            
            echo ""
            echo "═══════════════════════════════════════════════════════════════"
          fi
        fi
        
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Instance Information"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Remote: ${REMOTE_NAME}"
        echo "Name: ${INSTANCE_NAME_ARG}"
        echo "Status: ${VM_STATUS}"
        echo "Image: ${VM_IMAGE}"
        if [ -n "${VM_IP}" ]; then
          echo "IP Address: ${VM_IP}"
        else
          echo "IP Address: (not available yet - may need a moment to get DHCP lease)"
        fi
        
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Resource Allocation"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        if [ -n "${VM_MEMORY}" ]; then
          echo "Memory: ${VM_MEMORY}"
        fi
        if [ -n "${VM_CPU}" ]; then
          echo "CPU: ${VM_CPU} cores"
        fi
        if [ -n "${VM_DISK_SIZE}" ]; then
          echo "Disk: ${VM_DISK_SIZE}"
        fi
        
        # Get actual resource usage from inside the VM (if accessible)
        set +e  # Temporarily disable exit on error for resource info gathering
        if [ "${REMOTE_NAME}" != "local" ] && [ "${VM_STATUS}" = "Running" ]; then
          # Get memory usage
          MEMORY_USAGE=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- \
            free -h 2>/dev/null | grep "^Mem:" 2>/dev/null || echo "")
          if [ -n "${MEMORY_USAGE}" ]; then
            echo "Memory usage: ${MEMORY_USAGE}"
          fi
          
          # Get CPU count
          CPU_COUNT=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- \
            nproc 2>/dev/null || echo "")
          if [ -n "${CPU_COUNT}" ]; then
            echo "CPU (available): ${CPU_COUNT} cores"
          fi
          
          # Get disk usage
          DISK_INFO=$(incus exec "${REMOTE_NAME}:${INSTANCE_NAME_ARG}" -- \
            df -h / 2>/dev/null | tail -1 2>/dev/null || echo "")
          if [ -n "${DISK_INFO}" ]; then
            echo "Disk usage: ${DISK_INFO}"
          fi
        fi
        set -e  # Re-enable exit on error
        
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Access Information"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        if [ -n "${VM_IP}" ]; then
          echo "SSH: ssh ${CURRENT_USER}@${VM_IP}"
          echo "     Or: task vm:ssh -- ${INSTANCE_NAME_ARG}"
        else
          echo "SSH: task vm:ssh -- ${INSTANCE_NAME_ARG}"
        fi
        echo "Shell: task vm:shell -- ${INSTANCE_NAME_ARG}"
        echo "Info:  task vm:info -- ${INSTANCE_NAME_ARG}"
        
        # Workspace path if initialized
        if [ "{{.VM_INIT_WORKSPACE}}" = "true" ] && [ "${REMOTE_NAME}" != "local" ]; then
          WORKSPACE_NAME=$(basename "{{.WINDSOR_PROJECT_ROOT}}")
          WORKSPACE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
          echo "Workspace: ${WORKSPACE_PATH}"
        fi
        
        # Windsor Setup (skipped) message if flag not set
        if [ "${RUN_WINDSOR_UP}" != "true" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Windsor Setup (skipped)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "  Windsor setup was not requested (--windsor-up flag not set)"
          echo ""
          echo "  To run Windsor setup after VM creation, use:"
          echo "    task vm:create -- ${INSTANCE_NAME_ARG} --windsor-up"
          echo ""
          echo "  Or manually on the VM:"
          echo "    ssh ${CURRENT_USER}@${VM_IP:-<vm-ip>}"
          echo "    cd ~/$(basename "{{.WINDSOR_PROJECT_ROOT}}")"
          echo "    aqua install"
          echo "    windsor init local"
          echo "    windsor up"
        fi
        
        echo ""
        echo "═══════════════════════════════════════════════════════════════"
  
  create:validate:
    silent: true
    desc: Validate input and check prerequisites for instance creation
    cmds:
      - |
        set -euo pipefail
        
        # Validate required variables
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          echo "Add to contexts/<context>/windsor.yaml:"
          echo "  environment:"
          echo "    INCUS_REMOTE_NAME: <your-remote-name>"
          exit 1
        fi
        
        if [ -z "{{.WINDSOR_PROJECT_ROOT}}" ]; then
          echo "Error: WINDSOR_PROJECT_ROOT variable is not defined"
          echo "Run this from within a Windsor workspace"
          exit 1
        fi
        
        # Parse CLI arguments: [<type>] [<image>] [--name <name>]
        # All arguments are optional if environment variables are set
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        
        # Use defaults from environment variables
        IMAGE_ARG="{{.VM_IMAGE}}"
        IMAGE_ARG="${IMAGE_ARG:-ubuntu/24.04}"
        INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
        
        # Override with CLI arguments if provided
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          # First argument: image (optional)
          if [ $# -gt 0 ] && [ "${1}" != "--name" ]; then
            IMAGE_ARG="${1}"
            shift
          fi
          
          # Parse --name if provided
          while [ $# -gt 0 ]; do
            case "$1" in
              --name)
                INSTANCE_NAME_ARG="${2}"
                shift 2
                ;;
              *)
                shift
                ;;
            esac
          done
        fi
        
        # Set default instance name if not provided
        if [ -z "${INSTANCE_NAME_ARG}" ]; then
          INSTANCE_NAME_ARG="vm"
        fi
        
        # Check if instance already exists
        if incus list "{{.INCUS_REMOTE_NAME}}:" --format json 2>/dev/null | grep -q "\"name\":\"${INSTANCE_NAME_ARG}\""; then
          echo "Instance '${INSTANCE_NAME_ARG}' already exists on remote '{{.INCUS_REMOTE_NAME}}'"
          echo "Use 'task vm:delete -- ${INSTANCE_NAME_ARG}' to remove it first, or use a different name"
          exit 1
        fi

  generate-tfvars:
    silent: true
    desc: Generate terraform.tfvars from environment variables
    cmds:
      - |
        set -euo pipefail
        
        # Get environment variables with defaults
        INCUS_REMOTE_NAME="{{.INCUS_REMOTE_NAME}}"
        VM_NAME="{{.VM_INSTANCE_NAME}}"
        VM_NAME="${VM_NAME:-vm}"
        VM_IMAGE="{{.VM_IMAGE}}"
        VM_IMAGE="${VM_IMAGE:-ubuntu/24.04}"
        VM_MEMORY="{{.VM_MEMORY}}"
        VM_MEMORY="${VM_MEMORY:-8GB}"
        VM_CPU="{{.VM_CPU}}"
        VM_CPU="${VM_CPU:-4}"
        VM_AUTOSTART="{{.VM_AUTOSTART}}"
        VM_AUTOSTART="${VM_AUTOSTART:-false}"
        VM_DISK_SIZE="{{.VM_DISK_SIZE}}"
        PHYSICAL_NETWORK_NAME="{{.VM_NETWORK_NAME}}"
        STORAGE_POOL="{{.VM_STORAGE_POOL}}"
        STORAGE_POOL="${STORAGE_POOL:-local}"
        
        # Create terraform/vm directory if it doesn't exist
        TFVARS_DIR="terraform/vm"
        mkdir -p "${TFVARS_DIR}"
        TFVARS_FILE="${TFVARS_DIR}/terraform.tfvars"
        
        # Generate terraform.tfvars
        {
          printf "# Generated from environment variables - do not edit manually\n"
          printf "# Update environment variables in contexts/{{.WINDSOR_CONTEXT}}/windsor.yaml instead\n"
          printf "# To regenerate, run: task vm:generate-tfvars\n"
          printf "\n"
          printf "# Incus remote configuration\n"
          printf "incus_remote_name = \"${INCUS_REMOTE_NAME}\"\n"
          printf "\n"
          printf "# VM configuration\n"
          printf "vm_name = \"${VM_NAME}\"\n"
          printf "vm_image = \"${VM_IMAGE}\"\n"
          printf "vm_memory = \"${VM_MEMORY}\"\n"
          printf "vm_cpu = \"${VM_CPU}\"\n"
          printf "vm_autostart = ${VM_AUTOSTART}\n"
          if [ -n "${VM_DISK_SIZE}" ]; then
            printf "vm_disk_size = \"${VM_DISK_SIZE}\"\n"
          else
            printf "# vm_disk_size = \"\"  # Uses storage pool default if empty\n"
          fi
          printf "\n"
          printf "# Network configuration\n"
          if [ -n "${PHYSICAL_NETWORK_NAME}" ]; then
            printf "physical_network_name = \"${PHYSICAL_NETWORK_NAME}\"\n"
          else
            printf "# physical_network_name = \"\"  # Uses default Incus network if empty\n"
          fi
          printf "\n"
          printf "# Storage configuration\n"
          printf "storage_pool = \"${STORAGE_POOL}\"\n"
        } > "${TFVARS_FILE}"
        
        echo "✅ Generated ${TFVARS_FILE} from environment variables"
        echo ""
        echo "To regenerate, run: task vm:generate-tfvars"
        echo "To modify values, update environment variables in contexts/{{.WINDSOR_CONTEXT}}/windsor.yaml"

  terraform:init:
    silent: true
    desc: Initialize Terraform for the VM
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.WINDSOR_PROJECT_ROOT}}" ]; then
          echo "Error: WINDSOR_PROJECT_ROOT variable is not defined"
          echo "Run this from within a Windsor workspace"
          exit 1
        fi
        
        TERRAFORM_DIR="{{.WINDSOR_PROJECT_ROOT}}/terraform/vm"
        
        if [ ! -d "${TERRAFORM_DIR}" ]; then
          echo "Error: Terraform directory not found: ${TERRAFORM_DIR}"
          exit 1
        fi
        
        echo "Initializing Terraform..."
        cd "${TERRAFORM_DIR}"
        terraform init -upgrade

  terraform:plan:
    silent: true
    desc: Show Terraform plan for the VM
    cmds:
      - task: terraform:init
      - |
        set -euo pipefail
        
        TERRAFORM_DIR="{{.WINDSOR_PROJECT_ROOT}}/terraform/vm"
        cd "${TERRAFORM_DIR}"
        terraform plan

  terraform:apply:
    silent: true
    desc: Apply Terraform configuration to create VM
    cmds:
      - |
        set -euo pipefail
        
        TERRAFORM_DIR="{{.WINDSOR_PROJECT_ROOT}}/terraform/vm"
        cd "${TERRAFORM_DIR}"
        terraform apply -auto-approve

  terraform:destroy:
    silent: true
    desc: Destroy the VM using Terraform
    cmds:
      - |
        set -euo pipefail
        
        TERRAFORM_DIR="{{.WINDSOR_PROJECT_ROOT}}/terraform/vm"
        
        if [ ! -d "${TERRAFORM_DIR}" ]; then
          echo "Error: Terraform directory not found: ${TERRAFORM_DIR}"
          exit 1
        fi
        
        # Confirm deletion
        echo "⚠️  This will destroy the Ubuntu VM and all its data"
        echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
        sleep 5
        
        cd "${TERRAFORM_DIR}"
        terraform destroy -auto-approve
        echo "✅ VM destroyed"

  create:setup-env:
    silent: true
    desc: Setup developer environment in the instance
    cmds:
      - |
        set -euo pipefail
        
        # Parse instance name from CLI args
        INSTANCE_NAME_ARG="{{.CLI_ARGS}}"
        if [ -z "${INSTANCE_NAME_ARG}" ]; then
          echo "Error: Instance name required"
          exit 1
        fi
        
        # Ubuntu VM is always a VM
        INSTANCE_TYPE="vm"
        
        # Workspace is never mounted to /workspace
        # For remote deployments, use DEV_INIT_WORKSPACE=true to copy workspace contents to user's home directory
        # Workspace will be copied to ~/workspace-name (same folder name as on host)
        
        # Initialize developer environment for remote deployments
        if [ "{{.INCUS_REMOTE_NAME}}" != "local" ]; then
          echo ""
          echo "Setting up developer environment..."
          
          # Wait for VM agent to be ready if it's a VM
          if [ "${INSTANCE_TYPE}" = "vm" ]; then
            echo "  Ensuring VM agent is ready..."
            MAX_RETRIES=24
            RETRY_COUNT=0
            while [ ${RETRY_COUNT} -lt ${MAX_RETRIES} ]; do
              if incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- true 2>/dev/null; then
                echo "  VM agent is ready"
                break
              fi
              if [ $((RETRY_COUNT % 3)) -eq 0 ]; then
                echo "    Waiting for VM agent... (${RETRY_COUNT}/${MAX_RETRIES} attempts, ~$((RETRY_COUNT * 5))s elapsed)"
              fi
              sleep 5
              RETRY_COUNT=$((RETRY_COUNT + 1))
            done
            if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
              echo "⚠️  Error: VM agent not ready after $((MAX_RETRIES * 5)) seconds"
              echo "   The VM may still be booting. You can manually set up the environment later."
              exit 1
            fi
          fi
          
          # Get current user info from host
          CURRENT_USER=$(whoami)
          CURRENT_UID=$(id -u)
          CURRENT_GID=$(id -g)
          # Use HOME environment variable directly (most reliable)
          CURRENT_HOME="${HOME}"
          
          echo "  Current user: ${CURRENT_USER} (UID: ${CURRENT_UID}, GID: ${CURRENT_GID})"
          
          # Install essential developer tools
          echo "  Installing developer tools..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            apt-get update -qq
            apt-get install -y -qq \
              git \
              build-essential \
              curl \
              wget \
              vim \
              nano \
              openssh-client \
              openssh-server \
              sudo \
              ca-certificates \
              gnupg \
              lsb-release \
              > /dev/null 2>&1
          " || echo "⚠️  Warning: Some packages may have failed to install"
          
          # Install Incus on Ubuntu 24.04 LTS (native package available)
          echo "  Installing Incus (Ubuntu 24.04 LTS)..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            
            # Check if we're on Ubuntu 24.04 or later
            if [ -f /etc/os-release ]; then
              . /etc/os-release
              if [ \"\${ID}\" = \"ubuntu\" ] && [ -n \"\${VERSION_ID}\" ]; then
                # Check if version is 24.04 or later (e.g., 24.04, 24.10, etc.)
                MAJOR_VERSION=\$(echo \${VERSION_ID} | cut -d. -f1)
                MINOR_VERSION=\$(echo \${VERSION_ID} | cut -d. -f2)
                
                if [ \${MAJOR_VERSION} -gt 24 ] || ([ \${MAJOR_VERSION} -eq 24 ] && [ \${MINOR_VERSION} -ge 4 ]); then
                  echo \"Detected Ubuntu \${VERSION_ID}, installing Incus...\"
                  
                  # Install Incus (native package available in Ubuntu 24.04+)
                  apt-get update -qq
                  apt-get install -y -qq incus > /dev/null 2>&1
                  
                  # Install qemu-system for VM support (needed if running VMs inside the container/VM)
                  if [ \"${INSTANCE_TYPE}\" = \"vm\" ]; then
                    apt-get install -y -qq qemu-system > /dev/null 2>&1 || true
                  fi
                  
                  # Note: incus-tools (for LXD migration) is available but not installed by default
                  # Users can install it manually with: apt install incus-tools
                  
                  echo \"Incus installed successfully\"
                else
                  echo \"Ubuntu version \${VERSION_ID} detected (24.04+ required for native Incus package)\"
                  echo \"Skipping Incus installation\"
                fi
              else
                echo \"Not Ubuntu, skipping Incus installation\"
              fi
            else
              echo \"Unable to detect OS version, skipping Incus installation\"
            fi
          " || echo "⚠️  Warning: Incus installation may have failed"
          
          # Create user matching host user (must be done before installing tools that need user home directory)
          echo "  Creating user '${CURRENT_USER}'..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            if ! id -u '${CURRENT_USER}' >/dev/null 2>&1; then
              # Create group first (ignore if it exists)
              groupadd -g ${CURRENT_GID} '${CURRENT_USER}' 2>/dev/null || \
                (getent group ${CURRENT_GID} >/dev/null 2>&1 && echo 'Group with GID ${CURRENT_GID} already exists') || true
              # Create user with same UID/GID as host
              useradd -m -u ${CURRENT_UID} -g ${CURRENT_GID} -s /bin/bash '${CURRENT_USER}' 2>/dev/null || true
            fi
            
            # Ensure user is in sudo group (even if user already existed)
            usermod -aG sudo '${CURRENT_USER}' 2>/dev/null || true
            
            # Ensure passwordless sudo is configured (even if user already existed)
            echo '${CURRENT_USER} ALL=(ALL) NOPASSWD:ALL' > /etc/sudoers.d/${CURRENT_USER}
            chmod 0440 /etc/sudoers.d/${CURRENT_USER}
            
            # Verify user exists and get actual group name
            if id -u '${CURRENT_USER}' >/dev/null 2>&1; then
              USER_GROUP=\$(id -gn '${CURRENT_USER}' 2>/dev/null || echo '${CURRENT_USER}')
              # Create .ssh directory with proper ownership
              mkdir -p /home/${CURRENT_USER}/.ssh
              chmod 700 /home/${CURRENT_USER}/.ssh
              chown -R ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh 2>/dev/null || \
                chown -R '${CURRENT_USER}:'\${USER_GROUP} /home/${CURRENT_USER}/.ssh 2>/dev/null || true
              
              # Verify sudo configuration
              if groups '${CURRENT_USER}' | grep -q sudo; then
                echo \"User '${CURRENT_USER}' is in sudo group\"
              else
                echo \"⚠️  Warning: User '${CURRENT_USER}' may not be in sudo group\"
              fi
              if [ -f /etc/sudoers.d/${CURRENT_USER} ] && grep -q 'NOPASSWD' /etc/sudoers.d/${CURRENT_USER}; then
                echo \"Passwordless sudo configured for '${CURRENT_USER}'\"
              else
                echo \"⚠️  Warning: Passwordless sudo may not be configured\"
              fi
            fi
          "
          
          # Set up SSH keys (must be done before GitHub configuration)
          echo "  Setting up SSH keys..."
          
          # Find and copy all SSH keys (rsa, ed25519, ecdsa, etc.)
          SSH_KEYS_FOUND=0
          for key_type in rsa ed25519 ecdsa; do
            if [ -f "${CURRENT_HOME}/.ssh/id_${key_type}" ]; then
              SSH_KEYS_FOUND=1
              echo "    Copying ${key_type} key..."
              incus file push "${CURRENT_HOME}/.ssh/id_${key_type}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/id_${key_type}"
              incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                mv /tmp/id_${key_type} /home/${CURRENT_USER}/.ssh/id_${key_type}
                chmod 600 /home/${CURRENT_USER}/.ssh/id_${key_type}
                chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/id_${key_type} 2>/dev/null || true
              "
            fi
            if [ -f "${CURRENT_HOME}/.ssh/id_${key_type}.pub" ]; then
              incus file push "${CURRENT_HOME}/.ssh/id_${key_type}.pub" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/id_${key_type}.pub"
              incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                mv /tmp/id_${key_type}.pub /home/${CURRENT_USER}/.ssh/id_${key_type}.pub
                chmod 644 /home/${CURRENT_USER}/.ssh/id_${key_type}.pub
                chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/id_${key_type}.pub 2>/dev/null || true
              "
            fi
          done
          
          # Set up authorized_keys for passwordless SSH
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            # Ensure .ssh directory exists with correct permissions
            mkdir -p /home/${CURRENT_USER}/.ssh
            chmod 700 /home/${CURRENT_USER}/.ssh
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh 2>/dev/null || true
            
            # Ensure home directory has correct permissions (SSH requires home dir not be world-writable)
            chmod 755 /home/${CURRENT_USER} 2>/dev/null || true
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER} 2>/dev/null || true
            
            # Create or clear authorized_keys
            touch /home/${CURRENT_USER}/.ssh/authorized_keys
            chmod 600 /home/${CURRENT_USER}/.ssh/authorized_keys
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/authorized_keys 2>/dev/null || true
            
            # Add all public keys to authorized_keys
            for pubkey_file in /home/${CURRENT_USER}/.ssh/*.pub; do
              if [ -f \"\${pubkey_file}\" ]; then
                PUBKEY=\$(cat \"\${pubkey_file}\")
                if ! grep -Fxq \"\${PUBKEY}\" /home/${CURRENT_USER}/.ssh/authorized_keys 2>/dev/null; then
                  echo \"\${PUBKEY}\" >> /home/${CURRENT_USER}/.ssh/authorized_keys
                fi
              fi
            done
            
            # Ensure final permissions are correct
            chmod 600 /home/${CURRENT_USER}/.ssh/authorized_keys
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/authorized_keys 2>/dev/null || true
            chmod 700 /home/${CURRENT_USER}/.ssh
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh 2>/dev/null || true
          "
          
          if [ ${SSH_KEYS_FOUND} -eq 0 ]; then
            echo "⚠️  Warning: No SSH keys found in ${CURRENT_HOME}/.ssh/"
            echo "   You may need to generate SSH keys or manually add them to the VM"
          fi
          
          # Copy SSH config if it exists (filter out macOS-specific options)
          if [ -f "${CURRENT_HOME}/.ssh/config" ]; then
            echo "    Copying SSH config (filtering macOS-specific options)..."
            # Filter out macOS-specific SSH options that don't work on Linux
            # Create a filtered version by removing lines containing these options (case-insensitive)
            # Use awk for more reliable line-by-line processing
            awk '
              BEGIN { IGNORECASE=1 }
              !/usekeychain/ && !/addkeystoagent/ && !/useroamingservice/ { print }
            ' "${CURRENT_HOME}/.ssh/config" > /tmp/ssh_config_filtered 2>/dev/null || \
            grep -v -i -E 'usekeychain|addkeystoagent|useroamingservice' "${CURRENT_HOME}/.ssh/config" > /tmp/ssh_config_filtered 2>/dev/null || \
              cat "${CURRENT_HOME}/.ssh/config" > /tmp/ssh_config_filtered
            
            # Verify and clean the filtered file - remove any remaining problematic options
            # Use multiple passes to ensure all variations are removed
            for pass in 1 2 3; do
              # Remove lines containing these options (case-insensitive, any format)
              sed -i.bak -E '/[Uu][Ss][Ee][Kk][Ee][Yy][Cc][Hh][Aa][Ii][Nn]|[Aa][Dd][Dd][Kk][Ee][Yy][Ss][Tt][Oo][Aa][Gg][Ee][Nn][Tt]|[Uu][Ss][Ee][Rr][Oo][Aa][Mm][Ii][Nn][Gg][Ss][Ee][Rr][Vv][Ii][Cc][Ee]/d' /tmp/ssh_config_filtered 2>/dev/null || \
              sed -i.bak '/usekeychain/Id; /addkeystoagent/Id; /useroamingservice/Id' /tmp/ssh_config_filtered 2>/dev/null || true
              rm -f /tmp/ssh_config_filtered.bak 2>/dev/null || true
            done
            
            # Final verification
            if grep -i -q 'usekeychain\|addkeystoagent\|useroamingservice' /tmp/ssh_config_filtered 2>/dev/null; then
              echo "⚠️  Warning: Some macOS-specific options may still be present in SSH config"
              # Last resort: remove any line containing these words (case-insensitive)
              awk 'BEGIN { IGNORECASE=1 } !/usekeychain|addkeystoagent|useroamingservice/ { print }' /tmp/ssh_config_filtered > /tmp/ssh_config_filtered2 && \
                mv /tmp/ssh_config_filtered2 /tmp/ssh_config_filtered || true
            fi
            
            incus file push /tmp/ssh_config_filtered "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/ssh_config"
            incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
              mv /tmp/ssh_config /home/${CURRENT_USER}/.ssh/config
              chmod 600 /home/${CURRENT_USER}/.ssh/config
              chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/config 2>/dev/null || true
            "
            rm -f /tmp/ssh_config_filtered
          fi
          
          # Copy known_hosts if it exists (important for GitHub SSH)
          if [ -f "${CURRENT_HOME}/.ssh/known_hosts" ]; then
            incus file push "${CURRENT_HOME}/.ssh/known_hosts" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/known_hosts"
            incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
              mv /tmp/known_hosts /home/${CURRENT_USER}/.ssh/known_hosts
              chmod 644 /home/${CURRENT_USER}/.ssh/known_hosts
              chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/known_hosts 2>/dev/null || true
            "
          fi
          
          # Setup SSH agent and add keys (supports both passphrase-protected and unprotected keys)
          echo "  Setting up SSH agent..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            # Install openssh-client if not already installed (needed for ssh-add)
            if ! command -v ssh-add >/dev/null 2>&1; then
              apt-get update -qq
              apt-get install -y -qq openssh-client > /dev/null 2>&1 || true
            fi
            
            # Start SSH agent as the user in a persistent way
            # Use a script that starts the agent and adds keys
            su - ${CURRENT_USER} -c '
              # Start SSH agent and capture output
              eval \$(ssh-agent -s) > /dev/null 2>&1
              
              # Save agent environment to file
              echo \"export SSH_AUTH_SOCK=\${SSH_AUTH_SOCK}\" > ~/.ssh/agent_env
              echo \"export SSH_AGENT_PID=\${SSH_AGENT_PID}\" >> ~/.ssh/agent_env
              
              # Add all SSH keys to the agent (non-interactively)
              # For keys without passphrases, this will work immediately
              export SSH_ASKPASS=/bin/false
              export DISPLAY=:0
              for key_file in ~/.ssh/id_*; do
                if [ -f \"\${key_file}\" ] && [ ! -f \"\${key_file}.pub\" ]; then
                  # Private key file (not a .pub file)
                  # Try to add it non-interactively (will work if no passphrase)
                  ssh-add \"\${key_file}\" < /dev/null 2>/dev/null || true
                fi
              done
              
            ' || true
            
            # Configure SSH to use the agent and handle keys gracefully
            # Add to user's SSH config if not already present
            if ! grep -q 'AddKeysToAgent' /home/${CURRENT_USER}/.ssh/config 2>/dev/null; then
              echo '' >> /home/${CURRENT_USER}/.ssh/config
              echo '# Automatically add keys to agent' >> /home/${CURRENT_USER}/.ssh/config
              echo 'AddKeysToAgent yes' >> /home/${CURRENT_USER}/.ssh/config
              echo 'IdentitiesOnly yes' >> /home/${CURRENT_USER}/.ssh/config
            fi
            # Ensure SSH uses the agent by default (prefer agent over key files)
            if ! grep -q 'PreferredAuthentications' /home/${CURRENT_USER}/.ssh/config 2>/dev/null; then
              echo 'PreferredAuthentications publickey' >> /home/${CURRENT_USER}/.ssh/config
            fi
            
            # Ensure agent_env file has correct permissions
            chmod 600 /home/${CURRENT_USER}/.ssh/agent_env 2>/dev/null || true
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.ssh/agent_env 2>/dev/null || true
            
            echo \"SSH agent configured\"
          " || echo "⚠️  Warning: SSH agent setup may have failed"
          
          # Configure Git and GitHub credentials (before Homebrew installation)
          echo "  Configuring Git and GitHub credentials..."
          
          # Install git first if not already installed
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            if ! command -v git >/dev/null 2>&1; then
              apt-get update -qq
              apt-get install -y -qq git > /dev/null 2>&1 || true
            fi
          " || true
          
          # Configure git user info from host
          GIT_NAME=$(git config --global user.name 2>/dev/null || echo "")
          GIT_EMAIL=$(git config --global user.email 2>/dev/null || echo "")
          
          if [ -n "${GIT_NAME}" ] || [ -n "${GIT_EMAIL}" ]; then
            incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
              su - ${CURRENT_USER} -c 'git config --global user.name \"${GIT_NAME}\"' 2>/dev/null || true
              su - ${CURRENT_USER} -c 'git config --global user.email \"${GIT_EMAIL}\"' 2>/dev/null || true
            "
          fi
          
          # Configure git to use SSH for GitHub URLs instead of HTTPS (critical for Homebrew)
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            su - ${CURRENT_USER} -c 'git config --global url.\"git@github.com:\".insteadOf \"https://github.com/\"' 2>/dev/null || true
            su - ${CURRENT_USER} -c 'git config --global url.\"ssh://git@github.com/\".insteadOf \"https://github.com/\"' 2>/dev/null || true
            echo \"Git configured to use SSH for GitHub URLs\"
          " || true
          
          # Copy GitHub CLI config if it exists
          if [ -d "${CURRENT_HOME}/.config/gh" ]; then
            echo "    Copying GitHub CLI configuration..."
            incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
              mkdir -p /home/${CURRENT_USER}/.config/gh
              chown -R ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.config/gh 2>/dev/null || true
            " || true
            
            for gh_file in "${CURRENT_HOME}/.config/gh"/*; do
              if [ -f "${gh_file}" ]; then
                basename_gh=$(basename "${gh_file}")
                incus file push "${gh_file}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/gh_${basename_gh}" 2>/dev/null && \
                incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                  mv /tmp/gh_${basename_gh} /home/${CURRENT_USER}/.config/gh/${basename_gh}
                  chmod 600 /home/${CURRENT_USER}/.config/gh/${basename_gh} 2>/dev/null || true
                  chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.config/gh/${basename_gh} 2>/dev/null || true
                " || true
              elif [ -d "${gh_file}" ]; then
                basename_gh=$(basename "${gh_file}")
                incus file push -r "${gh_file}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/gh_${basename_gh}" 2>/dev/null && \
                incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                  cp -r /tmp/gh_${basename_gh}/* /home/${CURRENT_USER}/.config/gh/ 2>/dev/null || true
                  rm -rf /tmp/gh_${basename_gh}
                  chown -R ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.config/gh 2>/dev/null || true
                " || true
              fi
            done
          fi
          
          # Copy git credential helper config if it exists
          if [ -f "${CURRENT_HOME}/.git-credentials" ]; then
            echo "    Copying git credentials..."
            incus file push "${CURRENT_HOME}/.git-credentials" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}/tmp/git_credentials" && \
            incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
              mv /tmp/git_credentials /home/${CURRENT_USER}/.git-credentials
              chmod 600 /home/${CURRENT_USER}/.git-credentials
              chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.git-credentials 2>/dev/null || true
              su - ${CURRENT_USER} -c 'git config --global credential.helper store' 2>/dev/null || true
            " || true
          fi
          
          echo "  Git and GitHub credentials configured"
          
          # Install Docker (after user is created)
          echo "  Installing Docker..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            # Add Docker's official GPG key
            install -m 0755 -d /etc/apt/keyrings
            curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
            chmod a+r /etc/apt/keyrings/docker.gpg
            # Set up the repository
            echo \"deb [arch=\$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \$(. /etc/os-release && echo \"\$VERSION_CODENAME\") stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null
            # Update package lists
            apt-get update -qq
            
            # Install runc first (dependency of containerd)
            if ! dpkg -l | grep -q '^ii.*runc'; then
              apt-get install -y -qq runc 2>/dev/null || true
            fi
            
            # Install containerd from Ubuntu repos (to avoid Docker repo's broken containerd.io)
            if ! dpkg -l | grep -q '^ii.*containerd'; then
              apt-get install -y -qq containerd 2>/dev/null || true
            fi
            
            # Create apt preferences to prevent Docker repo's broken containerd.io from being installed
            mkdir -p /etc/apt/preferences.d
            echo 'Package: containerd.io' > /etc/apt/preferences.d/docker-containerd-pin
            echo 'Pin: release o=Docker' >> /etc/apt/preferences.d/docker-containerd-pin
            echo 'Pin-Priority: -1' >> /etc/apt/preferences.d/docker-containerd-pin
            apt-get update -qq
            
            # Always use workaround for docker-ce installation due to broken containerd.io in Docker repo
            # First, install Docker runtime dependencies (needed even with --force-depends)
            apt-get install -y -qq \
              libnftables1 \
              iptables \
              nftables \
              libip6tc2 \
              libnetfilter-conntrack3 \
              libnfnetlink0 \
              libnftnl11 \
              libjansson4 \
              apparmor \
              pigz \
              2>/dev/null || true
            
            # Install docker-ce-cli first (no containerd.io dependency)
            apt-get install -y -qq docker-ce-cli 2>/dev/null || true
            
            # Get docker-ce package version and install with workaround
            DOCKER_CE_PKG=\$(apt-cache madison docker-ce 2>/dev/null | head -1 | awk '{print \$3}' || echo \"\")
            if [ -n \"\${DOCKER_CE_PKG}\" ]; then
              # Download docker-ce package
              cd /tmp
              apt-get download docker-ce=\${DOCKER_CE_PKG} 2>/dev/null || true
              DOCKER_CE_DEB=\$(ls docker-ce_*.deb 2>/dev/null | head -1)
              if [ -n \"\${DOCKER_CE_DEB}\" ] && [ -f \"\${DOCKER_CE_DEB}\" ]; then
                # Install ignoring containerd.io dependency (Ubuntu's containerd is compatible)
                dpkg -i --force-depends \"\${DOCKER_CE_DEB}\" 2>/dev/null || true
                rm -f \"\${DOCKER_CE_DEB}\"
              fi
            fi
            
            # Install plugins (these don't require containerd.io)
            apt-get install -y -qq docker-buildx-plugin docker-compose-plugin 2>/dev/null || true
            
            # Check if systemd is available and running
            if systemctl is-system-running > /dev/null 2>&1 || [ -d /run/systemd/system ]; then
              # Use systemd (for VMs and containers with systemd)
              # Unmask and start containerd first
              systemctl unmask containerd 2>/dev/null || true
              systemctl daemon-reload 2>/dev/null || true
              systemctl enable containerd > /dev/null 2>&1 || true
              systemctl start containerd > /dev/null 2>&1 || true
              sleep 3
              
              # Verify containerd is running
              if systemctl is-active --quiet containerd 2>/dev/null; then
                echo \"containerd service is running\"
              else
                echo \"⚠️  Warning: containerd service may not be running\"
              fi
              
              # Unmask and start Docker service
              systemctl unmask docker 2>/dev/null || true
              systemctl daemon-reload 2>/dev/null || true
              
              # Enable Docker service (for auto-start on boot)
              systemctl enable docker > /dev/null 2>&1 || true
              
              # Verify service is enabled
              if systemctl is-enabled docker > /dev/null 2>&1; then
                echo \"Docker service enabled for auto-start\"
              fi
              
              # Start Docker service
              systemctl start docker > /dev/null 2>&1 || true
              sleep 4
              
              # Verify Docker is running
              if systemctl is-active --quiet docker 2>/dev/null; then
                echo \"Docker service is running\"
              else
                # Try restart with longer wait
                systemctl restart docker > /dev/null 2>&1 || true
                sleep 5
                if systemctl is-active --quiet docker 2>/dev/null; then
                  echo \"Docker service is running (after restart)\"
                else
                  # Check if there are any obvious errors
                  DOCKER_STATUS=\$(systemctl status docker --no-pager -l 2>&1 | head -15)
                  if echo \"\${DOCKER_STATUS}\" | grep -q 'masked'; then
                    echo \"⚠️  Warning: Docker service is masked, trying to unmask...\"
                    systemctl unmask docker 2>/dev/null || true
                    systemctl daemon-reload 2>/dev/null || true
                    systemctl start docker > /dev/null 2>&1 || true
                    sleep 3
                  fi
                  if systemctl is-active --quiet docker 2>/dev/null; then
                    echo \"Docker service is running (after unmask)\"
                  else
                    echo \"❌ ERROR: Docker service failed to start\"
                    echo \"   Service status:\"
                    systemctl status docker --no-pager -l 2>&1 | head -20
                    echo \"   Recent logs:\"
                    journalctl -u docker -n 30 --no-pager 2>&1 | tail -20
                    echo \"   You can manually start it with: sudo systemctl start docker\"
                    echo \"   Or check logs with: sudo journalctl -u docker -n 50\"
                    exit 1
                  fi
                fi
              fi
              
              # Final verification: Docker must be accessible
              sleep 2
              if ! docker info > /dev/null 2>&1 && ! sudo docker info > /dev/null 2>&1; then
                echo \"❌ ERROR: Docker daemon is not accessible\"
                echo \"   Service status: \$(systemctl is-active docker 2>&1 || echo 'unknown')\"
                echo \"   Socket exists: \$(test -S /var/run/docker.sock && echo 'yes' || echo 'no')\"
                exit 1
              fi
            else
              # Fallback for containers without systemd
              # Start containerd directly first
              if command -v containerd > /dev/null 2>&1; then
                containerd > /dev/null 2>&1 &
                sleep 2
              fi
              # Start dockerd directly
              if command -v dockerd > /dev/null 2>&1; then
                dockerd --containerd=/run/containerd/containerd.sock > /dev/null 2>&1 &
                sleep 3
              fi
            fi
            
            # Add user to docker group
            usermod -aG docker ${CURRENT_USER} 2>/dev/null || true
            
            # Verify Docker installation
            if command -v docker >/dev/null 2>&1; then
              # Give Docker a moment to fully start and verify
              sleep 2
              # Check if we can access docker (might need sudo)
              if docker info > /dev/null 2>&1 || sudo docker info > /dev/null 2>&1; then
                echo \"Docker installed and running\"
              else
                # Check service status for more info
                if systemctl is-active --quiet docker 2>/dev/null; then
                  echo \"⚠️  Warning: Docker service is running but 'docker info' failed (may need to add user to docker group)\"
                else
                  echo \"⚠️  Warning: Docker installed but daemon may not be running\"
                fi
              fi
            else
              echo \"⚠️  Warning: Docker binary not found\"
            fi
          " || echo "⚠️  Warning: Docker installation may have failed"
          
          # Configure br_netfilter kernel module for Kubernetes networking (required for Flannel CNI)
          echo "  Configuring br_netfilter kernel module..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            # Load br_netfilter module if not already loaded
            if ! lsmod | grep -q br_netfilter; then
              modprobe br_netfilter 2>/dev/null || echo \"⚠️  Warning: Failed to load br_netfilter module\"
            fi
            
            # Make module load on boot
            if [ ! -f /etc/modules-load.d/br_netfilter.conf ]; then
              echo 'br_netfilter' > /etc/modules-load.d/br_netfilter.conf
              chmod 644 /etc/modules-load.d/br_netfilter.conf
            fi
            
            # Set sysctls for bridge netfilter
            if [ ! -f /etc/sysctl.d/99-kubernetes.conf ]; then
              echo 'net.bridge.bridge-nf-call-iptables=1' > /etc/sysctl.d/99-kubernetes.conf
              echo 'net.bridge.bridge-nf-call-ip6tables=1' >> /etc/sysctl.d/99-kubernetes.conf
              chmod 644 /etc/sysctl.d/99-kubernetes.conf
            else
              # Add sysctls if file exists but doesn't have them
              if ! grep -q 'net.bridge.bridge-nf-call-iptables' /etc/sysctl.d/99-kubernetes.conf 2>/dev/null; then
                echo 'net.bridge.bridge-nf-call-iptables=1' >> /etc/sysctl.d/99-kubernetes.conf
              fi
              if ! grep -q 'net.bridge.bridge-nf-call-ip6tables' /etc/sysctl.d/99-kubernetes.conf 2>/dev/null; then
                echo 'net.bridge.bridge-nf-call-ip6tables=1' >> /etc/sysctl.d/99-kubernetes.conf
              fi
            fi
            
            # Apply sysctls immediately
            sysctl --system > /dev/null 2>&1 || true
            
            # Verify module is loaded and sysctls are set
            if lsmod | grep -q br_netfilter; then
              echo \"br_netfilter module loaded\"
            else
              echo \"⚠️  Warning: br_netfilter module not loaded (may not be available in kernel)\"
            fi
            
            if [ -f /proc/sys/net/bridge/bridge-nf-call-iptables ]; then
              IPTABLES_VAL=\$(cat /proc/sys/net/bridge/bridge-nf-call-iptables 2>/dev/null || echo '0')
              if [ \"\${IPTABLES_VAL}\" = \"1\" ]; then
                echo \"br_netfilter sysctls configured\"
              else
                echo \"⚠️  Warning: sysctls not set correctly\"
              fi
            else
              echo \"⚠️  Warning: /proc/sys/net/bridge/bridge-nf-call-iptables not found (module may not be available)\"
            fi
          " || echo "⚠️  Warning: br_netfilter configuration may have failed"
          
          # Install Homebrew (for Aqua, Windsor CLI and other tools) - must run as user, not root
          echo "  Installing Homebrew..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            # Check if Homebrew is already installed
            if [ -d /home/linuxbrew/.linuxbrew ] && [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then
              echo \"Homebrew already installed\"
            else
              # Install Homebrew as the user (not as root)
              # First install required dependencies
              apt-get update -qq
              apt-get install -y -qq build-essential curl file git > /dev/null 2>&1
              # Download installer script and run as user
              curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh -o /tmp/brew-install.sh
              chmod +x /tmp/brew-install.sh
              
              # Source SSH agent environment if available
              if [ -f /home/${CURRENT_USER}/.ssh/agent_env ]; then
                . /home/${CURRENT_USER}/.ssh/agent_env
              fi
              
              # Create a wrapper script to run Homebrew installer with proper environment
              # Temporarily remove SSH git config and use HTTPS for GitHub during Homebrew installation
              echo '#!/bin/bash' > /tmp/brew-install-wrapper.sh
              echo 'set +e' >> /tmp/brew-install-wrapper.sh
              echo 'set +u' >> /tmp/brew-install-wrapper.sh
              echo 'export DEBIAN_FRONTEND=noninteractive' >> /tmp/brew-install-wrapper.sh
              echo 'export NONINTERACTIVE=1' >> /tmp/brew-install-wrapper.sh
              echo '# Temporarily remove SSH git config to force HTTPS (avoids SSH passphrase prompts)' >> /tmp/brew-install-wrapper.sh
              echo 'git config --global --unset url.\"git@github.com:\".insteadOf 2>/dev/null || true' >> /tmp/brew-install-wrapper.sh
              echo 'git config --global --unset url.\"ssh://git@github.com/\".insteadOf 2>/dev/null || true' >> /tmp/brew-install-wrapper.sh
              echo '# Configure git to use HTTPS for GitHub' >> /tmp/brew-install-wrapper.sh
              echo 'git config --global url.\"https://github.com/\".insteadOf \"git@github.com:\" 2>/dev/null || true' >> /tmp/brew-install-wrapper.sh
              echo 'git config --global url.\"https://github.com/\".insteadOf \"ssh://git@github.com/\" 2>/dev/null || true' >> /tmp/brew-install-wrapper.sh
              echo '# Run Homebrew installer and capture output' >> /tmp/brew-install-wrapper.sh
              echo 'INSTALL_EXIT=1' >> /tmp/brew-install-wrapper.sh
              echo '/tmp/brew-install.sh < /dev/null > /tmp/brew-install-output.log 2>&1' >> /tmp/brew-install-wrapper.sh
              echo 'INSTALL_EXIT=$?' >> /tmp/brew-install-wrapper.sh
              echo '# Filter and show only important messages' >> /tmp/brew-install-wrapper.sh
              echo 'if grep -qiE \"Error|Failed\" /tmp/brew-install-output.log 2>/dev/null; then' >> /tmp/brew-install-wrapper.sh
              echo '  grep -iE \"Error|Failed\" /tmp/brew-install-output.log' >> /tmp/brew-install-wrapper.sh
              echo 'fi' >> /tmp/brew-install-wrapper.sh
              echo '# Check for success message (Homebrew installer can return non-zero even on success)' >> /tmp/brew-install-wrapper.sh
              echo 'if grep -q \"Installation successful\" /tmp/brew-install-output.log 2>/dev/null; then' >> /tmp/brew-install-wrapper.sh
              echo '  echo \"==> Installation successful\"' >> /tmp/brew-install-wrapper.sh
              echo '  echo \"Homebrew installed successfully\"' >> /tmp/brew-install-wrapper.sh
              echo 'elif [ \"${INSTALL_EXIT:-1}\" -eq 0 ]; then' >> /tmp/brew-install-wrapper.sh
              echo '  echo \"Homebrew installed successfully\"' >> /tmp/brew-install-wrapper.sh
              echo 'else' >> /tmp/brew-install-wrapper.sh
              echo '  echo \"⚠️  Homebrew installation failed (exit code: ${INSTALL_EXIT:-1})\"' >> /tmp/brew-install-wrapper.sh
              echo '  tail -20 /tmp/brew-install-output.log' >> /tmp/brew-install-wrapper.sh
              echo '  exit 1' >> /tmp/brew-install-wrapper.sh
              echo 'fi' >> /tmp/brew-install-wrapper.sh
              echo '# Restore git SSH configuration after installation' >> /tmp/brew-install-wrapper.sh
              echo 'git config --global --unset url.\"https://github.com/\".insteadOf 2>/dev/null || true' >> /tmp/brew-install-wrapper.sh
              echo 'git config --global url.\"git@github.com:\".insteadOf \"https://github.com/\" 2>/dev/null || true' >> /tmp/brew-install-wrapper.sh
              chmod +x /tmp/brew-install-wrapper.sh
              chown ${CURRENT_USER}:${CURRENT_GID} /tmp/brew-install-wrapper.sh 2>/dev/null || true
              
              # Run as user with proper environment including SSH agent
              sudo -u ${CURRENT_USER} -i /tmp/brew-install-wrapper.sh || {
                echo \"⚠️  Warning: Homebrew installation failed, will try direct Windsor CLI installation\"
                rm -f /tmp/brew-install.sh /tmp/brew-install-wrapper.sh
              }
              rm -f /tmp/brew-install.sh /tmp/brew-install-wrapper.sh
            fi
          " || echo "⚠️  Warning: Homebrew installation may have failed"
          
          # Install Aqua via Homebrew (after Homebrew is installed)
          echo "  Installing Aqua..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            set +e
            set +u
            export DEBIAN_FRONTEND=noninteractive
            if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then
              # Temporarily configure git to use HTTPS for GitHub to avoid SSH passphrase prompts
              sudo -u ${CURRENT_USER} -i bash -c 'set +e; set +u; git config --global --unset url.\"git@github.com:\".insteadOf 2>/dev/null || true; git config --global --unset url.\"ssh://git@github.com/\".insteadOf 2>/dev/null || true; git config --global url.\"https://github.com/\".insteadOf \"git@github.com:\" 2>/dev/null || true; eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" 2>/dev/null || true; /home/linuxbrew/.linuxbrew/bin/brew install aqua >/dev/null 2>&1; BREW_EXIT=\$?; git config --global --unset url.\"https://github.com/\".insteadOf 2>/dev/null || true; git config --global url.\"git@github.com:\".insteadOf \"https://github.com/\" 2>/dev/null || true; if [ \${BREW_EXIT} -eq 0 ] || [ -f /home/linuxbrew/.linuxbrew/bin/aqua ]; then exit 0; else exit 1; fi              ' && echo \"Aqua installed successfully via Homebrew\" || echo \"⚠️  Warning: Aqua installation via Homebrew failed\"
            else
              echo \"⚠️  Warning: Homebrew not available, skipping Aqua installation\"
            fi
          " || echo "⚠️  Warning: Aqua installation may have failed"
          
          # Install Windsor CLI (try Homebrew first, fallback to direct install)
          echo "  Installing Windsor CLI..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            set +e
            set +u
            export DEBIAN_FRONTEND=noninteractive
            WINDSOR_INSTALLED=0
            
            # Try Homebrew first - check if brew binary exists
            if [ -f /home/linuxbrew/.linuxbrew/bin/brew ]; then
              echo \"  Attempting installation via Homebrew...\"
              # Temporarily configure git to use HTTPS, install Windsor CLI, then restore
              sudo -u ${CURRENT_USER} -i bash -c 'set +e; set +u; git config --global --unset url.\"git@github.com:\".insteadOf 2>/dev/null || true; git config --global --unset url.\"ssh://git@github.com/\".insteadOf 2>/dev/null || true; git config --global url.\"https://github.com/\".insteadOf \"git@github.com:\" 2>/dev/null || true; git config --global url.\"https://github.com/\".insteadOf \"ssh://git@github.com/\" 2>/dev/null || true; eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\" 2>/dev/null || true; /home/linuxbrew/.linuxbrew/bin/brew update >/dev/null 2>&1; /home/linuxbrew/.linuxbrew/bin/brew tap windsorcli/cli >/dev/null 2>&1; /home/linuxbrew/.linuxbrew/bin/brew install windsor >/dev/null 2>&1; BREW_EXIT=\$?; git config --global --unset url.\"https://github.com/\".insteadOf 2>/dev/null || true; git config --global url.\"git@github.com:\".insteadOf \"https://github.com/\" 2>/dev/null || true; if [ \${BREW_EXIT} -eq 0 ] || [ -f /home/linuxbrew/.linuxbrew/bin/windsor ]; then exit 0; else exit 1; fi' 2>&1 && {
                if sudo -u ${CURRENT_USER} -i bash -c '/home/linuxbrew/.linuxbrew/bin/windsor version' > /dev/null 2>&1 || \
                   sudo -u ${CURRENT_USER} -i bash -c 'command -v windsor && windsor version' > /dev/null 2>&1; then
                  echo \"Windsor CLI installed successfully via Homebrew\"
                  WINDSOR_INSTALLED=1
                fi
              } || echo \"  ⚠️  Windsor CLI Homebrew installation failed\"
            fi
            
            # Fallback to direct installation if Homebrew installation failed
            if [ \${WINDSOR_INSTALLED} -eq 0 ]; then
              echo \"  Windsor CLI installation via Homebrew failed, installing directly...\"
              # Detect platform and architecture
              PLATFORM=\$(uname -s | tr '[:upper:]' '[:lower:]')
              ARCH=\$(uname -m)
              
              # Map architecture
              if [ \"\${ARCH}\" = \"x86_64\" ]; then
                ARCH=\"amd64\"
              elif [ \"\${ARCH}\" = \"arm64\" ] || [ \"\${ARCH}\" = \"aarch64\" ]; then
                ARCH=\"arm64\"
              fi
              
              # Install to /usr/local/bin for system-wide access
              INSTALL_DIR=\"/usr/local/bin\"
              WINDSOR_VERSION=\"v0.8.1\"
              
              # Download and install Windsor CLI
              DOWNLOAD_URL=\"https://github.com/windsorcli/cli/releases/download/\${WINDSOR_VERSION}/windsor_\${WINDSOR_VERSION#v}_\${PLATFORM}_\${ARCH}.tar.gz\"
              curl -fsSL \"\${DOWNLOAD_URL}\" | tar -xz -C \${INSTALL_DIR} windsor
              chmod +x \${INSTALL_DIR}/windsor
              
              # Verify installation
              if \${INSTALL_DIR}/windsor version > /dev/null 2>&1; then
                echo \"Windsor CLI installed successfully (direct install)\"
              else
                echo \"⚠️  Warning: Windsor CLI installation verification failed\"
              fi
            fi
          " || echo "⚠️  Warning: Windsor CLI installation may have failed"
          
          # Setup bashrc for user (PATH and shell configuration)
          echo "  Configuring bashrc..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            # Source SSH agent environment if it exists
            if [ -f /home/${CURRENT_USER}/.ssh/agent_env ] && ! grep -q 'agent_env' /home/${CURRENT_USER}/.bashrc 2>/dev/null; then
              echo '' >> /home/${CURRENT_USER}/.bashrc
              echo '# Source SSH agent environment' >> /home/${CURRENT_USER}/.bashrc
              echo 'if [ -f ~/.ssh/agent_env ]; then' >> /home/${CURRENT_USER}/.bashrc
              echo '  . ~/.ssh/agent_env > /dev/null 2>&1' >> /home/${CURRENT_USER}/.bashrc
              echo '  # Check if agent is still running, start if not' >> /home/${CURRENT_USER}/.bashrc
              echo '  if [ -n \"\${SSH_AGENT_PID}\" ] && ! ps -p \${SSH_AGENT_PID} > /dev/null 2>&1; then' >> /home/${CURRENT_USER}/.bashrc
              echo '    eval \$(ssh-agent -s) > /dev/null 2>&1' >> /home/${CURRENT_USER}/.bashrc
              echo '    echo \"SSH_AUTH_SOCK=\${SSH_AUTH_SOCK}\" > ~/.ssh/agent_env' >> /home/${CURRENT_USER}/.bashrc
              echo '    echo \"SSH_AGENT_PID=\${SSH_AGENT_PID}\" >> ~/.ssh/agent_env' >> /home/${CURRENT_USER}/.bashrc
              echo '    # Re-add keys to agent' >> /home/${CURRENT_USER}/.bashrc
              echo '    for key in ~/.ssh/id_*; do' >> /home/${CURRENT_USER}/.bashrc
              echo '      [ -f \"\${key}\" ] && [ ! -f \"\${key}.pub\" ] && ssh-add \"\${key}\" < /dev/null 2>/dev/null || true' >> /home/${CURRENT_USER}/.bashrc
              echo '    done' >> /home/${CURRENT_USER}/.bashrc
              echo '  fi' >> /home/${CURRENT_USER}/.bashrc
              echo 'fi' >> /home/${CURRENT_USER}/.bashrc
            fi
            
            # Add Homebrew to PATH (for Aqua, Windsor CLI and other tools) - only if brew binary exists
            if [ -f /home/linuxbrew/.linuxbrew/bin/brew ] && ! grep -q 'linuxbrew' /home/${CURRENT_USER}/.bashrc 2>/dev/null; then
              echo '' >> /home/${CURRENT_USER}/.bashrc
              echo '# Add Homebrew to PATH' >> /home/${CURRENT_USER}/.bashrc
              echo 'eval \"\$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"' >> /home/${CURRENT_USER}/.bashrc
            fi
            
            # Ensure /usr/local/bin is in PATH (though it's usually already there)
            if ! grep -q '/usr/local/bin' /home/${CURRENT_USER}/.bashrc 2>/dev/null; then
              echo '' >> /home/${CURRENT_USER}/.bashrc
              echo '# Ensure /usr/local/bin is in PATH' >> /home/${CURRENT_USER}/.bashrc
              echo 'export PATH=\"/usr/local/bin:\$PATH\"' >> /home/${CURRENT_USER}/.bashrc
            fi
            
            # Add Aqua bin directory to PATH (must be early to prioritize Aqua-installed tools)
            if ! grep -q 'AQUA_ROOT_DIR' /home/${CURRENT_USER}/.bashrc 2>/dev/null; then
              echo '' >> /home/${CURRENT_USER}/.bashrc
              echo '# Add Aqua bin directory to PATH (for Aqua-installed tools like terraform)' >> /home/${CURRENT_USER}/.bashrc
              echo 'export PATH=\"\${AQUA_ROOT_DIR:-\${XDG_DATA_HOME:-\$HOME/.local/share}/aquaproj-aqua}/bin:\${PATH}\"' >> /home/${CURRENT_USER}/.bashrc
            fi
            
            # Add Windsor hook (must be at the end, after rvm, git-prompt, and other prompt extensions)
            if ! grep -q 'windsor hook bash' /home/${CURRENT_USER}/.bashrc 2>/dev/null; then
              echo '' >> /home/${CURRENT_USER}/.bashrc
              echo '# Windsor CLI hook (must be after prompt extensions like rvm, git-prompt)' >> /home/${CURRENT_USER}/.bashrc
              echo 'eval \"\$(windsor hook bash)\"' >> /home/${CURRENT_USER}/.bashrc
            fi
            
            # Set ownership
            chown ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.bashrc 2>/dev/null || true
          " || echo "⚠️  Warning: bashrc configuration may have failed"
          
          # Create Windsor CLI config directory with proper permissions
          echo "  Setting up Windsor CLI configuration..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            mkdir -p /home/${CURRENT_USER}/.config/windsor
            chown -R ${CURRENT_UID}:${CURRENT_GID} /home/${CURRENT_USER}/.config 2>/dev/null || true
            chmod 755 /home/${CURRENT_USER}/.config 2>/dev/null || true
            chmod 755 /home/${CURRENT_USER}/.config/windsor 2>/dev/null || true
          " || echo "⚠️  Warning: Windsor CLI config directory setup may have failed"
          
          # Verify installations (check as user to get correct PATH)
          echo "  Verifying installations..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            echo 'Verification:'
            # Check Docker (as root)
            if command -v docker >/dev/null 2>&1; then
              DOCKER_VER=\$(docker --version 2>/dev/null | head -n1)
              # Check if Docker daemon is running
              if docker info > /dev/null 2>&1; then
                echo \"  ✓ Docker: \${DOCKER_VER} (running)\"
              else
                # Check if service is enabled/running
                if systemctl is-enabled docker > /dev/null 2>&1; then
                  if systemctl is-active --quiet docker 2>/dev/null; then
                    echo \"  ✓ Docker: \${DOCKER_VER} (service active, but docker info failed)\"
                  else
                    echo \"  ⚠ Docker: \${DOCKER_VER} (installed, service not active)\"
                  fi
                else
                  echo \"  ⚠ Docker: \${DOCKER_VER} (installed, service not enabled)\"
                fi
              fi
            else
              # Check if docker binary exists in standard location
              if [ -f /usr/bin/docker ]; then
                echo '  ⚠ Docker: binary exists at /usr/bin/docker but not in PATH'
              else
                echo '  ✗ Docker: not found'
              fi
            fi
            # Check Aqua and Windsor CLI as user (to get Homebrew PATH)
            # Source bashrc to ensure PATH is set correctly
            AQUA_CHECK=\$(su - ${CURRENT_USER} -c 'source ~/.bashrc 2>/dev/null || true; if [ -f /home/linuxbrew/.linuxbrew/bin/aqua ]; then /home/linuxbrew/.linuxbrew/bin/aqua --version 2>/dev/null | head -n1; elif command -v aqua >/dev/null 2>&1; then aqua --version 2>/dev/null | head -n1; else echo \"not found\"; fi' 2>/dev/null)
            if [ \"\${AQUA_CHECK}\" != \"not found\" ] && [ -n \"\${AQUA_CHECK}\" ]; then
              echo \"  ✓ Aqua: \${AQUA_CHECK}\"
            else
              # Check if binary exists even if not in PATH
              if [ -f /home/linuxbrew/.linuxbrew/bin/aqua ]; then
                AQUA_VER=\$(/home/linuxbrew/.linuxbrew/bin/aqua --version 2>/dev/null | head -n1 || echo \"installed but version check failed\")
                echo \"  ✓ Aqua: \${AQUA_VER} (binary exists, may need PATH setup)\"
              else
                echo '  ✗ Aqua: binary not found'
              fi
            fi
            WINDSOR_CHECK=\$(su - ${CURRENT_USER} -c 'source ~/.bashrc 2>/dev/null || true; if [ -f /home/linuxbrew/.linuxbrew/bin/windsor ]; then /home/linuxbrew/.linuxbrew/bin/windsor version 2>/dev/null | head -n1; elif command -v windsor >/dev/null 2>&1; then windsor version 2>/dev/null | head -n1; elif [ -f /usr/local/bin/windsor ]; then /usr/local/bin/windsor version 2>/dev/null | head -n1; else echo \"not found\"; fi' 2>/dev/null)
            if [ \"\${WINDSOR_CHECK}\" != \"not found\" ] && [ -n \"\${WINDSOR_CHECK}\" ]; then
              echo \"  ✓ Windsor CLI: \${WINDSOR_CHECK}\"
            else
              # Check if binary exists even if not in PATH
              if [ -f /home/linuxbrew/.linuxbrew/bin/windsor ]; then
                WINDSOR_VER=\$(/home/linuxbrew/.linuxbrew/bin/windsor version 2>/dev/null | head -n1 || echo \"installed but version check failed\")
                echo \"  ✓ Windsor CLI: \${WINDSOR_VER} (binary exists, may need PATH setup)\"
              elif [ -f /usr/local/bin/windsor ]; then
                WINDSOR_VER=\$(/usr/local/bin/windsor version 2>/dev/null | head -n1 || echo \"installed but version check failed\")
                echo \"  ✓ Windsor CLI: \${WINDSOR_VER} (installed to /usr/local/bin)\"
              else
                echo '  ✗ Windsor CLI: not found'
              fi
            fi
            echo ''
            echo 'Note: After SSH login, run \"source ~/.bashrc\" or restart your shell to load PATH changes'
            echo '      Aqua and Windsor CLI will be available via Homebrew PATH'
          " || true
          
          # Initialize workspace contents (AFTER user is created)
          INIT_WORKSPACE="{{.VM_INIT_WORKSPACE}}"
          INIT_WORKSPACE="${INIT_WORKSPACE:-false}"
          
          if [ "${INIT_WORKSPACE}" = "true" ]; then
            echo ""
            echo "Initializing workspace contents..."
            
            # Get workspace folder name from host path
            WORKSPACE_ROOT="{{.WINDSOR_PROJECT_ROOT}}"
            WORKSPACE_NAME=$(basename "${WORKSPACE_ROOT}")
            
            # Workspace is always copied to user's home directory with same name as on host
            INIT_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
            
            echo "  Source: ${WORKSPACE_ROOT}"
            echo "  Destination: ${INIT_PATH}"
            
            # Create the workspace directory in the VM (in user's home directory)
            incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
              mkdir -p ${INIT_PATH}
              chown -R ${CURRENT_UID}:${CURRENT_GID} ${INIT_PATH} 2>/dev/null || true
            "
            
            # Copy workspace contents using incus file push
            # Push contents of workspace root directly into INIT_PATH (not as a nested folder)
            if [ -d "${WORKSPACE_ROOT}" ]; then
              echo "Copying workspace contents..."
              # Use a more robust approach: iterate through all items in the workspace root
              # This handles hidden files and ensures all items are processed
              for item in "${WORKSPACE_ROOT}"/* "${WORKSPACE_ROOT}"/.[!.]* "${WORKSPACE_ROOT}"/..?*; do
                # Skip if glob didn't match anything
                [ -e "${item}" ] || continue
                
                basename_item=$(basename "${item}")
                echo "  Copying ${basename_item}..."
                
                # For files: push to the directory, incus will use the filename
                if [ -f "${item}" ]; then
                  if ! incus file push "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${INIT_PATH}/" 2>/dev/null; then
                    echo "⚠️  Warning: Failed to copy ${basename_item}"
                  fi
                elif [ -d "${item}" ]; then
                  # It's a directory - push the directory itself (not its contents) to avoid nesting
                  # Push the directory to the target location
                  if ! incus file push -r "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${INIT_PATH}/" 2>/dev/null; then
                    echo "⚠️  Warning: Failed to copy ${basename_item}"
                  fi
                  # Set ownership on the copied directory
                  incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                    chown -R ${CURRENT_UID}:${CURRENT_GID} ${INIT_PATH}/${basename_item} 2>/dev/null || true
                  " || true
                fi
              done
              
              # Set proper ownership on all copied files
              incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                chown -R ${CURRENT_UID}:${CURRENT_GID} ${INIT_PATH} 2>/dev/null || true
                # Ensure user has full access
                chmod -R u+rwX ${INIT_PATH} 2>/dev/null || true
              "
              
              echo "✅ Workspace contents initialized at ${INIT_PATH}"
            else
              echo "⚠️  Warning: Workspace root '${WORKSPACE_ROOT}' not found, skipping initialization"
            fi
          fi
          
          # Configure SSH server (openssh-server should already be installed with developer tools)
          echo "  Setting up SSH server..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            export DEBIAN_FRONTEND=noninteractive
            # Check if openssh-server is installed, if not try to install it
            if ! dpkg -l | grep -q '^ii.*openssh-server'; then
              echo 'openssh-server not found, attempting to install...'
              apt-get update -qq
              # Configure all packages first (helps with broken dependency state from Docker)
              dpkg --configure -a 2>/dev/null || true
              # Try installing openssh-server
              if ! apt-get install -y -qq openssh-server 2>&1; then
                echo 'Warning: Failed to install openssh-server (may be due to broken Docker dependencies)'
                echo 'You can manually install it later with: sudo apt-get install openssh-server'
              fi
            fi
            
            # Ensure SSH directory exists
            mkdir -p /etc/ssh
            mkdir -p /var/run/sshd
            
            # Check if config file exists, if not, generate it
            if [ ! -f /etc/ssh/sshd_config ]; then
              echo 'Generating SSH server configuration...'
              ssh-keygen -A 2>/dev/null || true
              # If still doesn't exist, create a minimal config
              if [ ! -f /etc/ssh/sshd_config ]; then
                dpkg-reconfigure -f noninteractive openssh-server 2>/dev/null || true
              fi
            fi
            
            # Backup original config if it exists
            if [ -f /etc/ssh/sshd_config ]; then
              cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak 2>/dev/null || true
              
              # Configure SSH for passwordless access
              # Disable password authentication
              sed -i 's/^#*PasswordAuthentication.*/PasswordAuthentication no/' /etc/ssh/sshd_config 2>/dev/null || true
              sed -i 's/^#*ChallengeResponseAuthentication.*/ChallengeResponseAuthentication no/' /etc/ssh/sshd_config 2>/dev/null || true
              # Enable public key authentication
              sed -i 's/^#*PubkeyAuthentication.*/PubkeyAuthentication yes/' /etc/ssh/sshd_config 2>/dev/null || true
              # Disable root login
              sed -i 's/^#*PermitRootLogin.*/PermitRootLogin no/' /etc/ssh/sshd_config 2>/dev/null || true
              
              # Ensure these settings are present (add if not found)
              grep -q '^PasswordAuthentication' /etc/ssh/sshd_config || echo 'PasswordAuthentication no' >> /etc/ssh/sshd_config
              grep -q '^PubkeyAuthentication' /etc/ssh/sshd_config || echo 'PubkeyAuthentication yes' >> /etc/ssh/sshd_config
              grep -q '^ChallengeResponseAuthentication' /etc/ssh/sshd_config || echo 'ChallengeResponseAuthentication no' >> /etc/ssh/sshd_config
            else
              echo 'Warning: /etc/ssh/sshd_config still does not exist after installation'
            fi
            
            # Enable and start SSH service
            systemctl enable ssh 2>/dev/null || systemctl enable sshd 2>/dev/null || true
            systemctl daemon-reload 2>/dev/null || true
            systemctl start ssh 2>/dev/null || systemctl start sshd 2>/dev/null || true
            # Wait a moment for SSH to start
            sleep 3
            
            # Verify SSH is running
            if systemctl is-active --quiet ssh 2>/dev/null || systemctl is-active --quiet sshd 2>/dev/null; then
              echo 'SSH server is running'
            else
              # Try to check what went wrong
              systemctl status ssh --no-pager -l 2>&1 | head -10 || systemctl status sshd --no-pager -l 2>&1 | head -10 || true
              echo 'Warning: SSH server may not be running'
            fi
          " || echo "⚠️  Warning: SSH server setup may have issues"
          
          # Verify SSH key setup
          echo "  Verifying SSH key setup..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            if [ -f /home/${CURRENT_USER}/.ssh/authorized_keys ]; then
              KEY_COUNT=\$(wc -l < /home/${CURRENT_USER}/.ssh/authorized_keys)
              echo \"    Found \${KEY_COUNT} key(s) in authorized_keys\"
              echo \"    authorized_keys permissions: \$(ls -l /home/${CURRENT_USER}/.ssh/authorized_keys | awk '{print \$1}')\"
              echo \"    .ssh directory permissions: \$(ls -ld /home/${CURRENT_USER}/.ssh | awk '{print \$1}')\"
            else
              echo \"    Warning: authorized_keys file not found\"
            fi
          " || true
          
          echo "✅ Developer environment initialized"
        fi

  start:
    silent: true
    desc: Start a dev instance
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        echo "Starting instance '${INSTANCE_NAME_ARG}'..."
        incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
        echo "✅ Instance started"

  stop:
    silent: true
    desc: Stop a dev instance
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        echo "Stopping instance '${INSTANCE_NAME_ARG}'..."
        incus stop "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
        echo "✅ Instance stopped"

  restart:
    silent: true
    desc: Restart a dev instance
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        echo "Restarting instance '${INSTANCE_NAME_ARG}'..."
        incus restart "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
        echo "✅ Instance restarted"

  shell:
    silent: true
    desc: Open an interactive shell in a dev instance (like docker exec -it)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Check if instance is running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Instance '${INSTANCE_NAME_ARG}' is not running. Starting it..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 2
        fi
        
        # Open interactive shell
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash

  exec:
    silent: true
    desc: Execute a command in a dev instance (like docker exec)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse CLI args - first arg might be instance name, rest is command
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -z "${CLI_ARGS_STR}" ]; then
          echo "Error: Command required"
          echo "Usage: task vm:exec -- <command>"
          echo "Usage: task vm:exec -- <instance-name> -- <command>"
          echo "Example: task vm:exec -- ls -la ~/workspace-name"
          echo "Example: task vm:exec -- my-VM -- apt update"
          exit 1
        fi
        
        eval set -- ${CLI_ARGS_STR}
        
        # Check if first arg looks like an instance name
        # Try to detect if first arg is an instance name by checking if it exists
        FIRST_ARG="${1}"
        if [ $# -gt 1 ] && [ "${2}" = "--" ]; then
          # Format: <instance-name> -- <command>
          INSTANCE_NAME_ARG="${FIRST_ARG}"
          shift 2
          COMMAND="$*"
        elif incus list "{{.INCUS_REMOTE_NAME}}:" --format csv -c n 2>/dev/null | grep -q "^${FIRST_ARG}$"; then
          # First arg is an instance name that exists
          INSTANCE_NAME_ARG="${FIRST_ARG}"
          shift
          COMMAND="$*"
        else
          # No instance name provided, use default
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
          COMMAND="$*"
        fi
        
        # Check if instance is running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Instance '${INSTANCE_NAME_ARG}' is not running. Starting it..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 2
        fi
        
        # Execute command
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "${COMMAND}"

  info:
    silent: true
    desc: Get information about a dev instance
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        echo "Instance: ${INSTANCE_NAME_ARG}"
        echo "Remote: {{.INCUS_REMOTE_NAME}}"
        echo ""
        incus info "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"

  debug:
    silent: true
    desc: Debug performance and resource usage of a dev instance
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Check if instance exists
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          echo "⚠️  Error: Instance '${INSTANCE_NAME_ARG}' does not exist on remote '{{.INCUS_REMOTE_NAME}}'"
          exit 1
        fi
        
        # Temporarily disable strict error checking for diagnostic commands
        set +e
        set +u
        
        echo "🔍 Performance Diagnostics for '${INSTANCE_NAME_ARG}'"
        echo "════════════════════════════════════════════════════════"
        echo ""
        
        # Instance status and type
        echo "📊 Instance Status:"
        INSTANCE_STATUS=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
          grep -oE '"status":"[^"]*"' | head -n 1 | cut -d'"' -f4 || echo "unknown")
        INSTANCE_TYPE=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
          grep -oE '"type":"[^"]*"' | head -n 1 | cut -d'"' -f4 || echo "unknown")
        echo "  Status: ${INSTANCE_STATUS}"
        echo "  Type: ${INSTANCE_TYPE}"
        echo ""
        
        # Resource limits from Incus
        echo "⚙️  Resource Limits (from Incus):"
        incus config show "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" 2>/dev/null | grep -E '^(limits\.|boot\.)' || echo "  (no limits configured)"
        echo ""
        
        # System load and uptime
        echo "📈 System Load (inside instance):"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'uptime && echo "" && cat /proc/loadavg 2>/dev/null || echo "  (unavailable)"' 2>/dev/null
        echo ""
        
        # CPU usage
        echo "🖥️  CPU Usage (top processes):"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'top -bn1 | head -n 20 2>/dev/null || ps aux --sort=-%cpu | head -n 10 2>/dev/null || echo "  (unavailable)"' 2>/dev/null
        echo ""
        
        # Memory usage
        echo "💾 Memory Usage:"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'free -h 2>/dev/null || cat /proc/meminfo | grep -E "^(MemTotal|MemAvailable|MemFree|SwapTotal|SwapFree)" 2>/dev/null || echo "  (unavailable)"' 2>/dev/null
        echo ""
        
        # Disk usage
        echo "💿 Disk Usage:"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'df -h / 2>/dev/null || echo "  (unavailable)"' 2>/dev/null
        echo ""
        
        # I/O wait and disk I/O
        echo "📊 I/O Statistics:"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'iostat -x 1 1 2>/dev/null || vmstat 1 1 2>/dev/null || echo "  (iostat/vmstat not available)"' 2>/dev/null
        echo ""
        
        # Network statistics
        echo "🌐 Network Statistics:"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'ss -s 2>/dev/null || netstat -s 2>/dev/null | head -n 20 || echo "  (unavailable)"' 2>/dev/null
        echo ""
        
        # Process count
        echo "🔢 Process Information:"
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'echo "  Total processes: $(ps aux 2>/dev/null | wc -l || echo "unknown")" && echo "  Running processes: $(ps aux 2>/dev/null | grep -c "[R] " || echo "unknown")"' 2>/dev/null
        echo ""
        
        # Network latency and connectivity
        echo "🌐 Network Performance:"
        echo "  Testing DNS resolution..."
        DNS_TIME=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'time (getent hosts github.com >/dev/null 2>&1 || host github.com >/dev/null 2>&1) 2>&1 | grep -oE "real.*[0-9.]+s" | head -n 1 || echo "unknown"' 2>/dev/null || echo "unknown")
        if [ "${DNS_TIME}" != "unknown" ]; then
          echo "  DNS resolution: ${DNS_TIME}"
        else
          echo "  DNS resolution: Testing..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            sh -c 'timeout 3 getent hosts github.com >/dev/null 2>&1 && echo "  ✅ DNS: Working" || echo "  ⚠️  DNS: Slow or failing"' 2>/dev/null || echo "  ⚠️  DNS: Unable to test"
        fi
        
        echo "  Testing connectivity to common hosts..."
        for host in github.com google.com 8.8.8.8; do
          PING_RESULT=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            sh -c "ping -c 1 -W 2 ${host} 2>/dev/null | grep -oE 'time=[0-9.]+' | head -n 1 || echo 'timeout'" 2>/dev/null || echo "failed")
          if [ "${PING_RESULT}" != "timeout" ] && [ "${PING_RESULT}" != "failed" ]; then
            echo "    ${host}: ${PING_RESULT}"
          else
            echo "    ${host}: ⚠️  Unreachable or slow"
          fi
        done
        echo ""
        
        # Disk write performance test
        echo "💿 Disk Performance:"
        # Check if bc is installed, install if needed
        if ! incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- command -v bc >/dev/null 2>&1; then
          echo "  Installing bc for performance tests..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            if command -v apt-get >/dev/null 2>&1; then
              apt-get update -qq >/dev/null 2>&1 && apt-get install -y -qq bc >/dev/null 2>&1
            elif command -v yum >/dev/null 2>&1; then
              yum install -y -q bc >/dev/null 2>&1
            elif command -v dnf >/dev/null 2>&1; then
              dnf install -y -q bc >/dev/null 2>&1
            fi
          " 2>/dev/null || true
        fi
        
        echo "  Testing write speed..."
        WRITE_TEST=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'WRITE_FILE=/tmp/disk_test_$$; \
            START=$(date +%s.%N); \
            dd if=/dev/zero of=${WRITE_FILE} bs=1M count=100 2>/dev/null; \
            END=$(date +%s.%N); \
            DURATION=$(echo "${END} - ${START}" | bc 2>/dev/null || echo "0"); \
            if [ "$(echo "${DURATION} > 0" | bc 2>/dev/null || echo 0)" = "1" ]; then \
              SPEED=$(echo "scale=2; 100 / ${DURATION}" | bc 2>/dev/null || echo "0"); \
              echo "${SPEED} MB/s"; \
            else \
              echo "test failed"; \
            fi; \
            rm -f ${WRITE_FILE} 2>/dev/null' 2>/dev/null || echo "unavailable")
        if [ "${WRITE_TEST}" != "unavailable" ] && [ "${WRITE_TEST}" != "test failed" ]; then
          echo "  Write speed: ${WRITE_TEST}"
        else
          echo "  Write speed: ⚠️  Unable to test (bc may not be installed)"
        fi
        
        echo "  Testing read speed..."
        READ_TEST=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'READ_FILE=/tmp/disk_read_test_$$; \
            dd if=/dev/zero of=${READ_FILE} bs=1M count=100 >/dev/null 2>&1; \
            START=$(date +%s.%N); \
            dd if=${READ_FILE} of=/dev/null bs=1M 2>/dev/null; \
            END=$(date +%s.%N); \
            DURATION=$(echo "${END} - ${START}" | bc 2>/dev/null || echo "0"); \
            if [ "$(echo "${DURATION} > 0" | bc 2>/dev/null || echo 0)" = "1" ]; then \
              SPEED=$(echo "scale=2; 100 / ${DURATION}" | bc 2>/dev/null || echo "0"); \
              echo "${SPEED} MB/s"; \
            else \
              echo "test failed"; \
            fi; \
            rm -f ${READ_FILE} 2>/dev/null' 2>/dev/null || echo "unavailable")
        if [ "${READ_TEST}" != "unavailable" ] && [ "${READ_TEST}" != "test failed" ]; then
          echo "  Read speed: ${READ_TEST}"
        else
          echo "  Read speed: ⚠️  Unable to test"
        fi
        
        # Check filesystem type and mount options
        echo "  Filesystem info:"
        FS_INFO=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'df -T / 2>/dev/null | tail -n 1 | awk "{print \"    Type: \" \$2 \", Mount: \" \$1}" || mount | grep " on / " | head -n 1' 2>/dev/null || echo "")
        if [ -n "${FS_INFO}" ]; then
          echo "${FS_INFO}"
        else
          echo "    (unavailable)"
        fi
        echo ""
        
        # Check for common performance issues
        echo "🔍 Common Issues Check:"
        
        # Check if swap is being used
        SWAP_USAGE=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'free | grep Swap | awk "{if (\$3 > 0) print \"⚠️  Swap is being used: \" \$3 \" KB\"}"' 2>/dev/null || echo "")
        if [ -n "${SWAP_USAGE}" ]; then
          echo "  ${SWAP_USAGE}"
        else
          echo "  ✅ Swap: Not in use"
        fi
        
        # Check disk space
        DISK_USAGE=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'df / | tail -n 1 | awk "{usage=\$5; gsub(/%/, \"\", usage); if (usage > 90) print \"⚠️  Disk usage: \" \$5 \" (\" \$3 \"/\" \$2 \")\"} else {print \"✅ Disk usage: \" \$5}}"' 2>/dev/null || echo "")
        if [ -n "${DISK_USAGE}" ]; then
          echo "  ${DISK_USAGE}"
        fi
        
        # Check load average
        LOAD_AVG=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
          sh -c 'cat /proc/loadavg 2>/dev/null | awk "{load1=\$1; if (load1 > 2.0) print \"⚠️  High load average: \" load1} else {print \"✅ Load average: \" load1}}"' 2>/dev/null || echo "")
        if [ -n "${LOAD_AVG}" ]; then
          echo "  ${LOAD_AVG}"
        fi
        
        echo ""
        echo "💡 Tips:"
        echo "  - High load: Check CPU-intensive processes with 'task vm:exec -- top'"
        echo "  - Memory issues: Check 'task vm:exec -- free -h'"
        echo "  - Disk I/O: Check 'task vm:exec -- iostat -x 1 5'"
        echo "  - Network issues: Check 'task vm:exec -- ping -c 5 github.com'"
        echo "  - Slow downloads: Check DNS with 'task vm:exec -- nslookup github.com'"
        echo "  - Increase resources: Edit limits with 'incus config set {{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG} limits.cpu=4 limits.memory=8GB'"
        echo ""
        echo "🔧 For slow 'aqua i' command:"
        echo "  - Check network speed: 'task vm:exec -- curl -o /dev/null -s -w \"%{speed_download}\" https://github.com'"
        echo "  - Check DNS: 'task vm:exec -- nslookup registry-aqua.slsa.dev'"
        echo "  - Monitor during install: 'task vm:exec -- watch -n 1 \"ps aux | grep aqua\"'"
        echo "  - Check disk I/O during install: 'task vm:exec -- iostat -x 1'"
        echo ""
        
        # Re-enable strict error checking
        set -e
        set -u

  ssh-info:
    desc: Show SSH connection information for the container
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Get instance name
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Check if instance exists
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          echo "⚠️  Error: Instance '${INSTANCE_NAME_ARG}' does not exist on remote '{{.INCUS_REMOTE_NAME}}'"
          echo ""
          echo "   Available instances:"
          incus list "{{.INCUS_REMOTE_NAME}}:" --format csv -c n 2>/dev/null | sed 's/^/     - /' || echo "     (none found)"
          echo ""
          echo "   Create an instance with:"
          echo "     task vm:create"
          exit 1
        fi
        
        # Check if instance is running
        INSTANCE_STATUS=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
          grep -oE '"status":"[^"]*"' | head -n 1 | cut -d'"' -f4 || echo "")
        
        if [ "${INSTANCE_STATUS}" != "Running" ]; then
          echo "⚠️  Warning: Instance '${INSTANCE_NAME_ARG}' is not running (status: ${INSTANCE_STATUS:-unknown})"
          echo "   Starting instance..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" 2>/dev/null || true
          echo "   Waiting for instance to be ready..."
          sleep 3
          
          # Wait for VM agent if it's a VM
          INSTANCE_TYPE=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
            grep -oE '"type":"[^"]*"' | head -n 1 | cut -d'"' -f4 || echo "container")
          if [ "${INSTANCE_TYPE}" = "virtual-machine" ]; then
            MAX_RETRIES=12
            RETRY_COUNT=0
            while [ ${RETRY_COUNT} -lt ${MAX_RETRIES} ]; do
              if incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- true 2>/dev/null; then
                break
              fi
              sleep 2
              RETRY_COUNT=$((RETRY_COUNT + 1))
            done
          fi
        fi
        
        # Get container IP address
        # Try to get IP from inside the container (most reliable)
        CONTAINER_IP=""
        
        echo "   Attempting to find IP address..."
        
        # Temporarily disable strict error checking for IP detection
        set +e
        set +u
        
        # Method 1: Try ip addr show eth0
        if [ -z "${CONTAINER_IP}" ]; then
          CONTAINER_IP=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            ip -4 addr show eth0 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
          if [ -n "${CONTAINER_IP}" ]; then
            echo "   ✓ Found IP via method 1 (ip addr show eth0): ${CONTAINER_IP}"
          fi
        fi
        
        # Method 2: Try hostname -I (get first non-loopback IP)
        if [ -z "${CONTAINER_IP}" ]; then
          ALL_IPS=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            hostname -I 2>/dev/null || echo "")
          if [ -n "${ALL_IPS}" ]; then
            # Get first IP that doesn't start with 127.
            for ip in ${ALL_IPS}; do
              if [ "${ip#127.}" != "${ip}" ]; then
                continue
              fi
              CONTAINER_IP="${ip}"
              echo "   ✓ Found IP via method 2 (hostname -I): ${CONTAINER_IP}"
              break
            done
          fi
        fi
        
        # Method 3: Try incus list JSON output (use awk for better compatibility)
        if [ -z "${CONTAINER_IP}" ]; then
          CONTAINER_IP=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
            grep -oE '"ipv4":\s*"[^"]*"' | head -n 1 | awk -F'"' '{print $4}' 2>/dev/null || echo "")
          if [ -n "${CONTAINER_IP}" ]; then
            echo "   ✓ Found IP via method 3 (incus list): ${CONTAINER_IP}"
          fi
        fi
        
        # Method 4: Directly parse ip addr show output (most reliable fallback)
        if [ -z "${CONTAINER_IP}" ]; then
          # Get all IPv4 addresses from ip addr show, skip loopback
          # Run the command and parse output on host side
          IP_OUTPUT=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- ip -4 addr show 2>/dev/null || echo "")
          if [ -n "${IP_OUTPUT}" ]; then
            ALL_IPS=$(echo "${IP_OUTPUT}" | grep -oE 'inet [0-9.]+' | awk '{print $2}' || echo "")
            if [ -n "${ALL_IPS}" ]; then
              # Find first IP that doesn't start with 127.
              for ip in ${ALL_IPS}; do
                if echo "${ip}" | grep -q '^127\.'; then
                  continue
                fi
                CONTAINER_IP="${ip}"
                echo "   ✓ Found IP via method 4 (ip addr show): ${CONTAINER_IP}"
                break
              done
            fi
          fi
        fi
        
        # Re-enable strict error checking
        set -e
        set -u
        
        # Method 5: Try to find any network interface by name
        if [ -z "${CONTAINER_IP}" ]; then
          # Get list of network interfaces - try multiple methods
          # Method 5a: From ip link show
          INTERFACES=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            ip link show 2>/dev/null | grep -oE '^[0-9]+:\s+[^:@]+' | awk '{print $2}' | grep -v '^lo$' 2>/dev/null || echo "")
          
          # Method 5b: If that fails, try from /sys/class/net
          if [ -z "${INTERFACES}" ]; then
            INTERFACES=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
              ls -1 /sys/class/net 2>/dev/null | grep -v '^lo$' 2>/dev/null || echo "")
          fi
          
          if [ -n "${INTERFACES}" ]; then
            for iface in ${INTERFACES}; do
              TEST_IP=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
                ip -4 addr show "${iface}" 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
              if [ -n "${TEST_IP}" ]; then
                CONTAINER_IP="${TEST_IP}"
                echo "   ✓ Found IP via method 5 (interface ${iface}): ${CONTAINER_IP}"
                break
              fi
            done
          fi
        fi
        
        # Get current user
        CURRENT_USER=$(whoami)
        
        if [ -z "${CONTAINER_IP}" ]; then
          echo "   ✗ All IP detection methods failed"
          echo ""
          echo "⚠️  Container IP not found. Is the container running?"
          echo ""
          echo "   Troubleshooting steps:"
          echo "   1. Check if instance is running:"
          echo "      incus list {{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          echo ""
          echo "   2. Start the instance if needed:"
          echo "      task vm:start -- ${INSTANCE_NAME_ARG}"
          echo ""
          echo "   3. Check network configuration:"
          echo "      incus network list {{.INCUS_REMOTE_NAME}}:"
          echo "      incus config show {{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG} | grep -i network"
          echo ""
          echo "   4. Check network interfaces inside the instance:"
          echo "      incus exec {{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG} -- ip addr show"
          exit 1
        fi
        
        echo ""
        echo "SSH Connection Information:"
        echo "  Container: ${INSTANCE_NAME_ARG}"
        echo "  IP Address: ${CONTAINER_IP}"
        echo "  User: ${CURRENT_USER}"
        echo ""
        echo "To SSH into the container:"
        echo "  ssh ${CURRENT_USER}@${CONTAINER_IP}"
        echo ""
        echo "Or use:"
        echo "  ssh ${CURRENT_USER}@${CONTAINER_IP} -t 'cd ~/workspace-name && bash'"
        echo ""
        echo "Note: Make sure the container has SSH server running and is accessible from your network."

  ssh:
    desc: SSH into the dev-vm (connects directly via SSH)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        # Get instance name
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Check if instance is running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Instance '${INSTANCE_NAME_ARG}' is not running. Starting it..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 3
        fi
        
        # Get container IP address
        # Try to get IP from inside the container (most reliable)
        CONTAINER_IP=""
        
        # Temporarily disable strict error checking for IP detection
        set +e
        set +u
        
        # Method 1: Try ip addr show eth0
        if [ -z "${CONTAINER_IP}" ]; then
          CONTAINER_IP=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            ip -4 addr show eth0 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
        fi
        
        # Method 2: Try hostname -I (get first non-loopback IP)
        if [ -z "${CONTAINER_IP}" ]; then
          ALL_IPS=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            hostname -I 2>/dev/null || echo "")
          if [ -n "${ALL_IPS}" ]; then
            # Get first IP that doesn't start with 127.
            for ip in ${ALL_IPS}; do
              if [ "${ip#127.}" != "${ip}" ]; then
                continue
              fi
              CONTAINER_IP="${ip}"
              break
            done
          fi
        fi
        
        # Method 3: Try incus list JSON output (use awk for better compatibility)
        if [ -z "${CONTAINER_IP}" ]; then
          CONTAINER_IP=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
            grep -oE '"ipv4":\s*"[^"]*"' | head -n 1 | awk -F'"' '{print $4}' 2>/dev/null || echo "")
        fi
        
        # Method 4: Directly parse ip addr show output (most reliable fallback)
        if [ -z "${CONTAINER_IP}" ]; then
          # Get all IPv4 addresses from ip addr show, skip loopback
          # Run the command and parse output on host side
          IP_OUTPUT=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- ip -4 addr show 2>/dev/null || echo "")
          if [ -n "${IP_OUTPUT}" ]; then
            ALL_IPS=$(echo "${IP_OUTPUT}" | grep -oE 'inet [0-9.]+' | awk '{print $2}' || echo "")
            if [ -n "${ALL_IPS}" ]; then
              # Find first IP that doesn't start with 127.
              for ip in ${ALL_IPS}; do
                if echo "${ip}" | grep -q '^127\.'; then
                  continue
                fi
                CONTAINER_IP="${ip}"
                break
              done
            fi
          fi
        fi
        
        # Method 5: Try to find any network interface by name
        if [ -z "${CONTAINER_IP}" ]; then
          # Get list of network interfaces - try multiple methods
          # Method 5a: From ip link show
          INTERFACES=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            ip link show 2>/dev/null | grep -oE '^[0-9]+:\s+[^:@]+' | awk '{print $2}' | grep -v '^lo$' 2>/dev/null || echo "")
          
          # Method 5b: If that fails, try from /sys/class/net
          if [ -z "${INTERFACES}" ]; then
            INTERFACES=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
              ls -1 /sys/class/net 2>/dev/null | grep -v '^lo$' 2>/dev/null || echo "")
          fi
          
          if [ -n "${INTERFACES}" ]; then
            for iface in ${INTERFACES}; do
              TEST_IP=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
                ip -4 addr show "${iface}" 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
              if [ -n "${TEST_IP}" ]; then
                CONTAINER_IP="${TEST_IP}"
                break
              fi
            done
          fi
        fi
        
        # Re-enable strict error checking
        set -e
        set -u
        
        # Get current user
        CURRENT_USER=$(whoami)
        
        if [ -z "${CONTAINER_IP}" ]; then
          echo "⚠️  Error: Could not determine IP address for '${INSTANCE_NAME_ARG}'"
          echo "   The instance may not have a network interface configured"
          echo "   Try: task vm:ssh-info -- ${INSTANCE_NAME_ARG}"
          exit 1
        fi
        
        # Validate IP address format
        if ! echo "${CONTAINER_IP}" | grep -qE '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$'; then
          echo "⚠️  Error: Invalid IP address format: ${CONTAINER_IP}"
          echo "   Try: task vm:ssh-info -- ${INSTANCE_NAME_ARG}"
          exit 1
        fi
        
        echo "📋 Connection Details:"
        echo "   Instance: ${INSTANCE_NAME_ARG}"
        echo "   IP Address: ${CONTAINER_IP}"
        echo "   User: ${CURRENT_USER}"
        echo ""
        
        # Test connectivity first
        echo "Testing connectivity to ${CONTAINER_IP}..."
        if ! ping -c 1 -W 2 ${CONTAINER_IP} >/dev/null 2>&1; then
          echo "⚠️  Warning: Cannot ping ${CONTAINER_IP}. SSH may still work if ICMP is blocked."
        else
          echo "✅ Ping successful"
        fi
        echo ""
        
        # SSH into the VM (don't exit on SSH failure, let user see the error)
        # Use -v flag for verbose output to help debug connection issues
        # Also use -o ConnectTimeout=10 to avoid hanging
        # Redirect stderr to stdout so verbose output is visible
        set +e
        ssh -v -o ConnectTimeout=10 ${CURRENT_USER}@${CONTAINER_IP} 2>&1
        SSH_EXIT_CODE=$?
        set -e
        
        if [ ${SSH_EXIT_CODE} -ne 0 ]; then
          echo ""
          echo "⚠️  SSH connection failed with exit code ${SSH_EXIT_CODE}"
          echo "   IP Address: ${CONTAINER_IP}"
          echo "   User: ${CURRENT_USER}"
          echo ""
          echo "   Common issues:"
          echo "   - SSH keys not set up (run: task vm:create to initialize the VM)"
          echo "   - Network connectivity issues (try: ping ${CONTAINER_IP})"
          echo "   - SSH server not running in the VM"
          echo "   - Firewall blocking port 22"
          echo ""
          echo "   Try: task vm:ssh-info -- ${INSTANCE_NAME_ARG} for more details"
        fi
        
        # Exit with the SSH exit code (0 for successful connection, non-zero for failure)
        exit ${SSH_EXIT_CODE}
  
  list:
    silent: true
    desc: List all dev instances
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        echo "Ubuntu VMs on remote '{{.INCUS_REMOTE_NAME}}':"
        echo ""
        incus list "{{.INCUS_REMOTE_NAME}}:" --format table

  delete:
    silent: true
    desc: "Delete the VM using Terraform. Optionally specify VM name: task vm:delete [-- <vm-name>]"
    cmds:
      - |
        set -euo pipefail
        
        TERRAFORM_DIR="{{.WINDSOR_PROJECT_ROOT}}/terraform/vm"
        TFVARS_FILE="${TERRAFORM_DIR}/terraform.tfvars"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        
        if [ -z "${REMOTE}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        if [ ! -d "${TERRAFORM_DIR}" ]; then
          echo "Error: Terraform directory not found: ${TERRAFORM_DIR}"
          exit 1
        fi
        
        # Parse instance name from CLI args or use default
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
          echo "Using VM name from argument: ${INSTANCE_NAME_ARG}"
          USE_VAR_FLAG=true
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
          echo "Using VM name from environment: ${INSTANCE_NAME_ARG}"
          USE_VAR_FLAG=false
        fi
        
        # Confirm deletion
        echo "⚠️  This will destroy the VM '${INSTANCE_NAME_ARG}' and all its data"
        echo "Press Ctrl+C to cancel, or wait 5 seconds to continue..."
        sleep 5
        
        # Check if VM exists in Incus
        VM_EXISTS=false
        if incus list "${REMOTE}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          VM_EXISTS=true
        fi
        
        # Try Terraform destroy first
        TERRAFORM_SUCCESS=false
        cd "${TERRAFORM_DIR}"
        set +e  # Temporarily disable exit on error for Terraform
        if [ "${USE_VAR_FLAG}" = "true" ]; then
          # Use -var flag to override vm_name when provided via CLI
          terraform destroy -auto-approve -var "vm_name=${INSTANCE_NAME_ARG}" > /tmp/terraform_destroy.log 2>&1
        else
          # Use terraform.tfvars when using environment default
          terraform destroy -auto-approve > /tmp/terraform_destroy.log 2>&1
        fi
        TERRAFORM_EXIT=$?
        set -e  # Re-enable exit on error
        
        # Check if Terraform actually destroyed something
        if [ ${TERRAFORM_EXIT} -eq 0 ]; then
          if grep -q "Resources: [1-9]" /tmp/terraform_destroy.log 2>/dev/null; then
            TERRAFORM_SUCCESS=true
            echo "✅ VM '${INSTANCE_NAME_ARG}' destroyed via Terraform"
          fi
        fi
        
        # If Terraform didn't destroy it (or VM wasn't in Terraform state), delete directly via Incus
        if [ "${TERRAFORM_SUCCESS}" = "false" ] && [ "${VM_EXISTS}" = "true" ]; then
          echo "⚠️  VM not found in Terraform state, deleting directly via Incus..."
          if incus delete "${REMOTE}:${INSTANCE_NAME_ARG}" --force > /dev/null 2>&1; then
            echo "✅ VM '${INSTANCE_NAME_ARG}' destroyed via Incus"
          else
            echo "❌ Failed to delete VM '${INSTANCE_NAME_ARG}'"
            exit 1
          fi
        elif [ "${TERRAFORM_SUCCESS}" = "false" ] && [ "${VM_EXISTS}" = "false" ]; then
          echo "⚠️  VM '${INSTANCE_NAME_ARG}' not found in Terraform state or Incus"
          echo "   (It may have already been deleted)"
        fi

  init-workspace:
    desc: Initialize workspace contents in an existing VM (copies workspace to user's home directory)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        if [ "{{.INCUS_REMOTE_NAME}}" = "local" ]; then
          echo "Error: Workspace initialization is only for remote VMs"
          echo "   For local instances, the workspace can be accessed directly from the host filesystem"
          exit 1
        fi
        
        # Get instance name
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Get current user info from host
        CURRENT_USER=$(whoami)
        CURRENT_UID=$(id -u)
        CURRENT_GID=$(id -g)
        
        # Get workspace folder name from host path
        WORKSPACE_ROOT="{{.WINDSOR_PROJECT_ROOT}}"
        WORKSPACE_NAME=$(basename "${WORKSPACE_ROOT}")
        
        # Workspace is always copied to user's home directory with same name as on host
        INIT_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
        
        echo "Initializing workspace contents for '${INSTANCE_NAME_ARG}'..."
        echo "  Source: ${WORKSPACE_ROOT}"
        echo "  Destination: ${INIT_PATH}"
        
        # Check if instance exists and is running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          echo "Error: Instance '${INSTANCE_NAME_ARG}' does not exist"
          exit 1
        fi
        
        # Start instance if not running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Starting instance..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 3
        fi
        
        # Wait for VM agent to be ready if it's a VM
        INSTANCE_TYPE=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -o '"type":"[^"]*"' | cut -d'"' -f4 || echo "container")
        if [ "${INSTANCE_TYPE}" = "virtual-machine" ]; then
          echo "  Waiting for VM agent to be ready..."
          MAX_RETRIES=24
          RETRY_COUNT=0
          while [ ${RETRY_COUNT} -lt ${MAX_RETRIES} ]; do
            if incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- true 2>/dev/null; then
              echo "  VM agent is ready"
              break
            fi
            sleep 2
            RETRY_COUNT=$((RETRY_COUNT + 1))
          done
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo "⚠️  Error: VM agent not ready after $((MAX_RETRIES * 2)) seconds"
            exit 1
          fi
        fi
        
        # Create the workspace directory in the VM (in user's home directory)
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
          mkdir -p ${INIT_PATH}
          chown -R ${CURRENT_UID}:${CURRENT_GID} ${INIT_PATH} 2>/dev/null || true
        "
        
        # Copy workspace contents using incus file push
        if [ -d "${WORKSPACE_ROOT}" ]; then
          echo "Copying workspace contents..."
          # Use a more robust approach: iterate through all items in the workspace root
          # This handles hidden files and ensures all items are processed
          for item in "${WORKSPACE_ROOT}"/* "${WORKSPACE_ROOT}"/.[!.]* "${WORKSPACE_ROOT}"/..?*; do
            # Skip if glob didn't match anything
            [ -e "${item}" ] || continue
            
            basename_item=$(basename "${item}")
            echo "  Copying ${basename_item}..."
            
            # For files: push to the directory
            if [ -f "${item}" ]; then
              if ! incus file push "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${INIT_PATH}/" 2>/dev/null; then
                echo "⚠️  Warning: Failed to copy ${basename_item}"
              fi
            elif [ -d "${item}" ]; then
              # It's a directory - push the directory itself (not its contents) to avoid nesting
              # Push the directory to the target location
              if ! incus file push -r "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${INIT_PATH}/" 2>/dev/null; then
                echo "⚠️  Warning: Failed to copy ${basename_item}"
              fi
              # Set ownership on the copied directory
              incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                chown -R ${CURRENT_UID}:${CURRENT_GID} ${INIT_PATH}/${basename_item} 2>/dev/null || true
              " || true
            fi
          done
          
          # Set proper ownership on all copied files
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            chown -R ${CURRENT_UID}:${CURRENT_GID} ${INIT_PATH} 2>/dev/null || true
            chmod -R u+rwX ${INIT_PATH} 2>/dev/null || true
          "
          
          echo "✅ Workspace contents initialized at ${INIT_PATH}"
        else
          echo "⚠️  Error: Workspace root '${WORKSPACE_ROOT}' not found"
          exit 1
        fi

  copy-workspace:
    desc: Copy entire workspace to VM (replaces existing workspace in user's home directory)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        if [ -z "{{.WINDSOR_PROJECT_ROOT}}" ]; then
          echo "Error: WINDSOR_PROJECT_ROOT variable is not defined"
          echo "Run this from within a Windsor workspace"
          exit 1
        fi
        
        # Get instance name
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Get current user info from host
        CURRENT_USER=$(whoami)
        CURRENT_UID=$(id -u)
        CURRENT_GID=$(id -g)
        
        # Get workspace folder name from host path
        WORKSPACE_ROOT="{{.WINDSOR_PROJECT_ROOT}}"
        WORKSPACE_NAME=$(basename "${WORKSPACE_ROOT}")
        
        # Workspace is copied to user's home directory with same name as on host
        TARGET_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
        
        echo "Copying workspace to '${INSTANCE_NAME_ARG}'..."
        echo "  Source: ${WORKSPACE_ROOT}"
        echo "  Destination: ${TARGET_PATH}"
        
        # Check if instance exists
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          echo "Error: Instance '${INSTANCE_NAME_ARG}' does not exist"
          exit 1
        fi
        
        # Start instance if not running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Starting instance..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 3
        fi
        
        # Wait for VM agent to be ready if it's a VM
        INSTANCE_TYPE=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -o '"type":"[^"]*"' | cut -d'"' -f4 || echo "container")
        if [ "${INSTANCE_TYPE}" = "virtual-machine" ]; then
          echo "  Waiting for VM agent to be ready..."
          MAX_RETRIES=24
          RETRY_COUNT=0
          while [ ${RETRY_COUNT} -lt ${MAX_RETRIES} ]; do
            if incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- true 2>/dev/null; then
              echo "  VM agent is ready"
              break
            fi
            sleep 2
            RETRY_COUNT=$((RETRY_COUNT + 1))
          done
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo "⚠️  Error: VM agent not ready after $((MAX_RETRIES * 2)) seconds"
            exit 1
          fi
        fi
        
        # Remove existing workspace directory if it exists (to ensure clean copy)
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
          if [ -d ${TARGET_PATH} ]; then
            rm -rf ${TARGET_PATH}
          fi
          mkdir -p ${TARGET_PATH}
          chown -R ${CURRENT_UID}:${CURRENT_GID} ${TARGET_PATH} 2>/dev/null || true
        "
        
        # Copy workspace contents
        if [ -d "${WORKSPACE_ROOT}" ]; then
          echo "Copying workspace contents..."
          for item in "${WORKSPACE_ROOT}"/* "${WORKSPACE_ROOT}"/.[!.]* "${WORKSPACE_ROOT}"/..?*; do
            [ -e "${item}" ] || continue
            
            basename_item=$(basename "${item}")
            echo "  Copying ${basename_item}..."
            
            if [ -f "${item}" ]; then
              if ! incus file push "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${TARGET_PATH}/" 2>/dev/null; then
                echo "⚠️  Warning: Failed to copy ${basename_item}"
              fi
            elif [ -d "${item}" ]; then
              if ! incus file push -r "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${TARGET_PATH}/" 2>/dev/null; then
                echo "⚠️  Warning: Failed to copy ${basename_item}"
              fi
              incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                chown -R ${CURRENT_UID}:${CURRENT_GID} ${TARGET_PATH}/${basename_item} 2>/dev/null || true
              " || true
            fi
          done
          
          # Set proper ownership
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            chown -R ${CURRENT_UID}:${CURRENT_GID} ${TARGET_PATH} 2>/dev/null || true
            chmod -R u+rwX ${TARGET_PATH} 2>/dev/null || true
          "
          
          echo "✅ Workspace copied to ${TARGET_PATH}"
        else
          echo "⚠️  Error: Workspace root '${WORKSPACE_ROOT}' not found"
          exit 1
        fi

  add-workspace:
    desc: Add workspace to VM (copies workspace to user's home directory, same as during creation)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        if [ -z "{{.WINDSOR_PROJECT_ROOT}}" ]; then
          echo "Error: WINDSOR_PROJECT_ROOT variable is not defined"
          echo "Run this from within a Windsor workspace"
          exit 1
        fi
        
        # Get instance name
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Get current user info from host
        CURRENT_USER=$(whoami)
        CURRENT_UID=$(id -u)
        CURRENT_GID=$(id -g)
        
        # Get workspace folder name from host path
        WORKSPACE_ROOT="{{.WINDSOR_PROJECT_ROOT}}"
        WORKSPACE_NAME=$(basename "${WORKSPACE_ROOT}")
        
        # Workspace is copied to user's home directory with same name as on host
        TARGET_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
        
        echo "Adding workspace to '${INSTANCE_NAME_ARG}'..."
        echo "  Source: ${WORKSPACE_ROOT}"
        echo "  Destination: ${TARGET_PATH}"
        
        # Check if instance exists
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          echo "Error: Instance '${INSTANCE_NAME_ARG}' does not exist"
          exit 1
        fi
        
        # Start instance if not running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Starting instance..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 3
        fi
        
        # Wait for VM agent to be ready if it's a VM
        INSTANCE_TYPE=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -o '"type":"[^"]*"' | cut -d'"' -f4 || echo "container")
        if [ "${INSTANCE_TYPE}" = "virtual-machine" ]; then
          echo "  Waiting for VM agent to be ready..."
          MAX_RETRIES=24
          RETRY_COUNT=0
          while [ ${RETRY_COUNT} -lt ${MAX_RETRIES} ]; do
            if incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- true 2>/dev/null; then
              echo "  VM agent is ready"
              break
            fi
            sleep 2
            RETRY_COUNT=$((RETRY_COUNT + 1))
          done
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo "⚠️  Error: VM agent not ready after $((MAX_RETRIES * 2)) seconds"
            exit 1
          fi
        fi
        
        # Create workspace directory (merge with existing if it exists)
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
          mkdir -p ${TARGET_PATH}
          chown -R ${CURRENT_UID}:${CURRENT_GID} ${TARGET_PATH} 2>/dev/null || true
        "
        
        # Copy workspace contents (will merge with existing files)
        if [ -d "${WORKSPACE_ROOT}" ]; then
          echo "Copying workspace contents..."
          for item in "${WORKSPACE_ROOT}"/* "${WORKSPACE_ROOT}"/.[!.]* "${WORKSPACE_ROOT}"/..?*; do
            [ -e "${item}" ] || continue
            
            basename_item=$(basename "${item}")
            echo "  Copying ${basename_item}..."
            
            if [ -f "${item}" ]; then
              if ! incus file push "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${TARGET_PATH}/" 2>/dev/null; then
                echo "⚠️  Warning: Failed to copy ${basename_item}"
              fi
            elif [ -d "${item}" ]; then
              if ! incus file push -r "${item}" "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}${TARGET_PATH}/" 2>/dev/null; then
                echo "⚠️  Warning: Failed to copy ${basename_item}"
              fi
              incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
                chown -R ${CURRENT_UID}:${CURRENT_GID} ${TARGET_PATH}/${basename_item} 2>/dev/null || true
              " || true
            fi
          done
          
          # Set proper ownership
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            chown -R ${CURRENT_UID}:${CURRENT_GID} ${TARGET_PATH} 2>/dev/null || true
            chmod -R u+rwX ${TARGET_PATH} 2>/dev/null || true
          "
          
          echo "✅ Workspace added to ${TARGET_PATH}"
        else
          echo "⚠️  Error: Workspace root '${WORKSPACE_ROOT}' not found"
          exit 1
        fi

  sync-workspace:
    desc: Sync workspace changes to VM (uploads only changed files using rsync)
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.INCUS_REMOTE_NAME}}" ]; then
          echo "Error: INCUS_REMOTE_NAME variable is not defined"
          exit 1
        fi
        
        if [ -z "{{.WINDSOR_PROJECT_ROOT}}" ]; then
          echo "Error: WINDSOR_PROJECT_ROOT variable is not defined"
          echo "Run this from within a Windsor workspace"
          exit 1
        fi
        
        # Get instance name
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -n "${CLI_ARGS_STR}" ]; then
          eval set -- ${CLI_ARGS_STR}
          INSTANCE_NAME_ARG="${1}"
        else
          if [ -n "{{.VM_INSTANCE_NAME}}" ]; then
            INSTANCE_NAME_ARG="{{.VM_INSTANCE_NAME}}"
          else
            INSTANCE_NAME_ARG="vm"
          fi
        fi
        
        # Get current user info from host
        CURRENT_USER=$(whoami)
        
        # Get workspace folder name from host path
        WORKSPACE_ROOT="{{.WINDSOR_PROJECT_ROOT}}"
        WORKSPACE_NAME=$(basename "${WORKSPACE_ROOT}")
        
        # Workspace is in user's home directory with same name as on host
        REMOTE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
        
        echo "Syncing workspace to '${INSTANCE_NAME_ARG}'..."
        echo "  Source: ${WORKSPACE_ROOT}"
        echo "  Destination: ${CURRENT_USER}@<ip>:${REMOTE_PATH}"
        echo ""
        
        # Check if instance exists
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format csv -c n 2>/dev/null | grep -q "^${INSTANCE_NAME_ARG}$"; then
          echo "⚠️  Error: Instance '${INSTANCE_NAME_ARG}' does not exist on remote '{{.INCUS_REMOTE_NAME}}'"
          exit 1
        fi
        
        # Start instance if not running
        if ! incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -q '"status":"Running"'; then
          echo "Starting instance..."
          incus start "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}"
          sleep 3
        fi
        
        # Wait for VM agent to be ready if it's a VM
        INSTANCE_TYPE=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | grep -o '"type":"[^"]*"' | cut -d'"' -f4 || echo "container")
        if [ "${INSTANCE_TYPE}" = "virtual-machine" ]; then
          echo "  Waiting for VM agent to be ready..."
          MAX_RETRIES=24
          RETRY_COUNT=0
          while [ ${RETRY_COUNT} -lt ${MAX_RETRIES} ]; do
            if incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- true 2>/dev/null; then
              echo "  VM agent is ready"
              break
            fi
            sleep 2
            RETRY_COUNT=$((RETRY_COUNT + 1))
          done
          if [ ${RETRY_COUNT} -ge ${MAX_RETRIES} ]; then
            echo "⚠️  Error: VM agent not ready after $((MAX_RETRIES * 2)) seconds"
            exit 1
          fi
        fi
        
        # Get container IP address
        CONTAINER_IP=""
        
        # Temporarily disable strict error checking for IP detection
        set +e
        set +u
        
        # Method 1: Try ip addr show eth0
        if [ -z "${CONTAINER_IP}" ]; then
          CONTAINER_IP=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            ip -4 addr show eth0 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
        fi
        
        # Method 2: Try hostname -I (get first non-loopback IP)
        if [ -z "${CONTAINER_IP}" ]; then
          ALL_IPS=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            hostname -I 2>/dev/null || echo "")
          if [ -n "${ALL_IPS}" ]; then
            # Get first IP that doesn't start with 127.
            for ip in ${ALL_IPS}; do
              if [ "${ip#127.}" != "${ip}" ]; then
                continue
              fi
              CONTAINER_IP="${ip}"
              break
            done
          fi
        fi
        
        # Method 3: Try incus list JSON output
        if [ -z "${CONTAINER_IP}" ]; then
          CONTAINER_IP=$(incus list "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" --format json 2>/dev/null | \
            grep -oE '"ipv4":\s*"[^"]*"' | head -n 1 | awk -F'"' '{print $4}' 2>/dev/null || echo "")
        fi
        
        # Method 4: Directly parse ip addr show output
        if [ -z "${CONTAINER_IP}" ]; then
          IP_OUTPUT=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- ip -4 addr show 2>/dev/null || echo "")
          if [ -n "${IP_OUTPUT}" ]; then
            ALL_IPS=$(echo "${IP_OUTPUT}" | grep -oE 'inet [0-9.]+' | awk '{print $2}' || echo "")
            if [ -n "${ALL_IPS}" ]; then
              for ip in ${ALL_IPS}; do
                if echo "${ip}" | grep -q '^127\.'; then
                  continue
                fi
                CONTAINER_IP="${ip}"
                break
              done
            fi
          fi
        fi
        
        # Method 5: Try to find any network interface by name
        if [ -z "${CONTAINER_IP}" ]; then
          INTERFACES=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
            ip link show 2>/dev/null | grep -oE '^[0-9]+:\s+[^:@]+' | awk '{print $2}' | grep -v '^lo$' 2>/dev/null || echo "")
          if [ -z "${INTERFACES}" ]; then
            INTERFACES=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
              ls -1 /sys/class/net 2>/dev/null | grep -v '^lo$' 2>/dev/null || echo "")
          fi
          if [ -n "${INTERFACES}" ]; then
            for iface in ${INTERFACES}; do
              TEST_IP=$(incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- \
                ip -4 addr show "${iface}" 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
              if [ -n "${TEST_IP}" ]; then
                CONTAINER_IP="${TEST_IP}"
                break
              fi
            done
          fi
        fi
        
        # Re-enable strict error checking
        set -e
        set -u
        
        if [ -z "${CONTAINER_IP}" ]; then
          echo "⚠️  Error: Could not determine IP address for instance '${INSTANCE_NAME_ARG}'"
          echo ""
          echo "   Troubleshooting:"
          echo "   1. Check if instance is running: task vm:list"
          echo "   2. Start the instance: task vm:start -- ${INSTANCE_NAME_ARG}"
          echo "   3. Check network: task vm:ssh-info -- ${INSTANCE_NAME_ARG}"
          exit 1
        fi
        
        echo "  Instance IP: ${CONTAINER_IP}"
        echo ""
        
        # Check if rsync is available
        if ! command -v rsync >/dev/null 2>&1; then
          echo "⚠️  Error: rsync is not installed on the host"
          echo "   Install it with: brew install rsync (macOS) or apt-get install rsync (Linux)"
          exit 1
        fi
        
        # Check if rsync is available in the remote instance
        if ! incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- command -v rsync >/dev/null 2>&1; then
          echo "⚠️  Warning: rsync not found in instance. Installing..."
          incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
            if command -v apt-get >/dev/null 2>&1; then
              apt-get update -qq && apt-get install -y -qq rsync
            elif command -v yum >/dev/null 2>&1; then
              yum install -y -q rsync
            elif command -v dnf >/dev/null 2>&1; then
              dnf install -y -q rsync
            else
              echo 'Error: Cannot install rsync automatically'
              exit 1
            fi
          " || {
            echo "⚠️  Error: Failed to install rsync in the instance"
            exit 1
          }
        fi
        
        # Ensure remote directory exists
        echo "  Ensuring remote directory exists..."
        incus exec "{{.INCUS_REMOTE_NAME}}:${INSTANCE_NAME_ARG}" -- bash -c "
          mkdir -p ${REMOTE_PATH}
          chown -R ${CURRENT_USER}:${CURRENT_USER} ${REMOTE_PATH} 2>/dev/null || true
        " || true
        
        # Perform rsync sync
        echo "  Syncing files..."
        echo ""
        
        # Build rsync command
        # -a: archive mode (preserves permissions, timestamps, etc.)
        # -v: verbose
        # -z: compress during transfer
        # --delete: remove files that don't exist locally (optional, but good for sync)
        # --exclude: exclude common build artifacts and dependencies
        RSYNC_CMD="rsync -avz --delete \
          --exclude='.git/' \
          --exclude='node_modules/' \
          --exclude='.venv/' \
          --exclude='venv/' \
          --exclude='__pycache__/' \
          --exclude='*.pyc' \
          --exclude='.pytest_cache/' \
          --exclude='.mypy_cache/' \
          --exclude='dist/' \
          --exclude='build/' \
          --exclude='.tox/' \
          --exclude='.coverage' \
          --exclude='.DS_Store' \
          --exclude='*.swp' \
          --exclude='*.swo' \
          --exclude='*~' \
          --exclude='.idea/' \
          --exclude='.vscode/' \
          -e 'ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10' \
          '${WORKSPACE_ROOT}/' \
          '${CURRENT_USER}@${CONTAINER_IP}:${REMOTE_PATH}/'"
        
        # Execute rsync
        eval "${RSYNC_CMD}"
        RSYNC_EXIT_CODE=$?
        
        if [ ${RSYNC_EXIT_CODE} -eq 0 ]; then
          echo ""
          echo "✅ Workspace synced successfully to ${REMOTE_PATH}"
        else
          echo ""
          echo "⚠️  Warning: rsync exited with code ${RSYNC_EXIT_CODE}"
          echo "   Some files may not have been synced. Check the output above for errors."
          exit ${RSYNC_EXIT_CODE}
        fi

  test:
    desc: Test Ubuntu VM setup by running through all runbook steps and validating the VM. By default, initializes workspace. Use --no-workspace to skip. Use --keep to leave VM running after test. Use --windsor-up to also run windsor init and windsor up.
    cmds:
      - |
        set -euo pipefail
        
        # Parse CLI args: <incus-remote-name> [--keep] [--no-workspace] [--windsor-up]
        CLI_ARGS_STR="{{.CLI_ARGS}}"
        if [ -z "${CLI_ARGS_STR}" ]; then
          echo "Error: INCUS_REMOTE_NAME is required"
          echo "Usage: task vm:test -- <incus-remote-name> [--keep] [--no-workspace] [--windsor-up]"
          echo ""
          echo "Options:"
          echo "  --keep, --no-cleanup    Keep VM running after test (default: delete VM)"
          echo "  --no-workspace          Skip workspace initialization (default: initialize workspace)"
          echo "  --windsor-up            Run windsor init and windsor up after workspace setup"
          echo ""
          echo "Examples:"
          echo "  task vm:test -- nuc"
          echo "  task vm:test -- nuc --keep  # Keep VM after test"
          echo "  task vm:test -- nuc --no-workspace  # Skip workspace initialization"
          echo "  task vm:test -- nuc --windsor-up  # Also run windsor up"
          echo "  task vm:test -- nuc --keep --windsor-up  # Keep VM and run windsor up"
          exit 1
        fi
        
        # Initialize flags
        SKIP_CLEANUP=false
        SKIP_WORKSPACE=false
        RUN_WINDSOR_UP=false
        
        # Parse arguments
        eval set -- ${CLI_ARGS_STR}
        TEST_REMOTE_NAME="${1}"
        shift || true
        
        # Check for flags
        while [ $# -gt 0 ]; do
          case "${1}" in
            --keep|--no-cleanup)
              SKIP_CLEANUP=true
              shift
              ;;
            --no-workspace)
              SKIP_WORKSPACE=true
              shift
              ;;
            --windsor-up)
              RUN_WINDSOR_UP=true
              shift
              ;;
            *)
              echo "⚠️  Warning: Unknown argument '${1}', ignoring"
              shift
              ;;
          esac
        done
        
        # Get VM name from environment or use default
        VM_NAME="{{.VM_INSTANCE_NAME}}"
        VM_NAME="${VM_NAME:-vm}"
        
        # Get current user
        CURRENT_USER=$(whoami)
        
        # Get project root
        PROJECT_ROOT="{{.WINDSOR_PROJECT_ROOT}}"
        if [ -z "${PROJECT_ROOT}" ]; then
          PROJECT_ROOT="$(pwd)"
        fi
        
        # Test results tracking
        TESTS_PASSED=0
        TESTS_FAILED=0
        FAILED_TESTS=()
        
        echo "═══════════════════════════════════════════════════════════════"
        echo "Ubuntu VM Setup Test"
        echo "═══════════════════════════════════════════════════════════════"
        echo ""
        echo "Remote: ${TEST_REMOTE_NAME}"
        echo "VM Name: ${VM_NAME}"
        echo "User: ${CURRENT_USER}"
        echo ""
        
        # Step 0: Initialize Windsor context "test"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Step 0: Initialize Windsor Context"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        
        # Create contexts directory if it doesn't exist
        CONTEXTS_DIR="${PROJECT_ROOT}/contexts"
        TEST_CONTEXT_DIR="${CONTEXTS_DIR}/test"
        TEST_WINDSOR_YAML="${TEST_CONTEXT_DIR}/windsor.yaml"
        
        mkdir -p "${TEST_CONTEXT_DIR}"
        
        # Get default values for environment variables
        VM_IMAGE="${VM_IMAGE:-ubuntu/24.04}"
        VM_MEMORY="${VM_MEMORY:-16GB}"  # Increased for Docker workloads
        VM_CPU="${VM_CPU:-4}"
        VM_DISK_SIZE="${VM_DISK_SIZE:-100GB}"  # Increased for Docker workloads
        
        # Default to physical network (eno1) if available, otherwise use empty (defaults to bridge)
        if [ -z "${VM_NETWORK_NAME:-}" ]; then
          # Check if eno1 network exists on the remote
          if incus network show "${TEST_REMOTE_NAME}:eno1" >/dev/null 2>&1; then
            VM_NETWORK_NAME="eno1"
            echo "  Detected physical network 'eno1', using it for direct network attachment"
          else
            VM_NETWORK_NAME=""
          fi
        else
          VM_NETWORK_NAME="${VM_NETWORK_NAME}"
        fi
        
        VM_STORAGE_POOL="${VM_STORAGE_POOL:-local}"
        VM_AUTOSTART="${VM_AUTOSTART:-false}"
        VM_DISK_SIZE="${VM_DISK_SIZE:-100GB}"  # Ensure it's set
        
        # Set workspace initialization: true by default, unless --no-workspace flag is set
        # Force to true if flag is not set, regardless of existing environment variable
        if [ "${SKIP_WORKSPACE}" = "true" ]; then
          VM_INIT_WORKSPACE="false"
        else
          # Explicitly set to true (don't use environment variable, as it may be from previous run)
          VM_INIT_WORKSPACE="true"
        fi
        
        DOCKER_HOST_VAL="${DOCKER_HOST:-unix:///var/run/docker.sock}"
        
        # Create or update windsor.yaml with environment variables (matching Step 2 of vm.md)
        {
          echo "id: test-VM"
          echo "provider: generic"
          echo "environment:"
          echo "  # Use remote Incus server"
          echo "  INCUS_REMOTE_NAME: ${TEST_REMOTE_NAME}"
          echo ""
          echo "  # Workspace initialization (optional, defaults to true in test task)"
          echo "  VM_INIT_WORKSPACE: ${VM_INIT_WORKSPACE}  # Set to true to initialize workspace on creation"
          echo ""
          echo "  # Default VM name (optional, defaults to vm)"
          echo "  VM_INSTANCE_NAME: ${VM_NAME}"
          echo ""
          echo "  # Default image (optional, defaults to ubuntu/24.04)"
          echo "  VM_IMAGE: ${VM_IMAGE}"
          echo ""
          echo "  # VM resources (optional)"
          echo "  VM_MEMORY: ${VM_MEMORY}"
          echo "  VM_CPU: ${VM_CPU}"
          if [ -n "${VM_DISK_SIZE}" ]; then
          echo "  VM_DISK_SIZE: ${VM_DISK_SIZE}"
          fi
          echo ""
          echo "  # Network interface for VM (optional, leave empty to use default Incus network)"
          echo "  # This should be a physical interface on the IncusOS server for direct network access"
          if [ -n "${VM_NETWORK_NAME}" ]; then
            echo "  VM_NETWORK_NAME: ${VM_NETWORK_NAME}"
          else
            echo "  # VM_NETWORK_NAME: \"\"  # Uses default Incus network if empty"
          fi
          echo ""
          echo "  # Storage pool (optional, defaults to local)"
          echo "  VM_STORAGE_POOL: ${VM_STORAGE_POOL}"
          echo ""
          echo "  # Auto-start VM on host boot (optional, defaults to false)"
          echo "  VM_AUTOSTART: ${VM_AUTOSTART}"
          echo ""
          echo "  # Use default Docker socket (VMs run Docker natively)"
          echo "  DOCKER_HOST: ${DOCKER_HOST_VAL}"
        } > "${TEST_WINDSOR_YAML}"
        
        echo "✅ Created/updated ${TEST_WINDSOR_YAML}"
        
        # Set Windsor context to "test"
        if command -v windsor > /dev/null 2>&1; then
          if windsor context set test > /dev/null 2>&1; then
            echo "✅ Set Windsor context to 'test'"
            TESTS_PASSED=$((TESTS_PASSED + 1))
          else
            # Try to initialize the context if setting fails
            if windsor init --context test --backend local --config-dir "${TEST_CONTEXT_DIR}" > /dev/null 2>&1; then
              echo "✅ Initialized and set Windsor context to 'test'"
              TESTS_PASSED=$((TESTS_PASSED + 1))
            else
              echo "⚠️  Warning: Could not set Windsor context, but continuing with environment variables"
              TESTS_FAILED=$((TESTS_FAILED + 1))
            fi
          fi
        else
          echo "⚠️  Warning: 'windsor' command not found, but continuing with environment variables"
          TESTS_FAILED=$((TESTS_FAILED + 1))
        fi
        
        # Report environment variable values
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Environment Variables Configuration"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "INCUS_REMOTE_NAME: ${TEST_REMOTE_NAME}"
        echo "VM_INSTANCE_NAME: ${VM_NAME}"
        echo "VM_IMAGE: ${VM_IMAGE}"
        echo "VM_MEMORY: ${VM_MEMORY}"
        echo "VM_CPU: ${VM_CPU}"
        echo "VM_DISK_SIZE: ${VM_DISK_SIZE}"
        if [ -n "${VM_NETWORK_NAME}" ]; then
          echo "VM_NETWORK_NAME: ${VM_NETWORK_NAME}"
        else
          echo "VM_NETWORK_NAME: (not set, will use default Incus network)"
        fi
        echo "VM_STORAGE_POOL: ${VM_STORAGE_POOL}"
        echo "VM_AUTOSTART: ${VM_AUTOSTART}"
        echo "VM_INIT_WORKSPACE: ${VM_INIT_WORKSPACE}"
        echo "DOCKER_HOST: ${DOCKER_HOST_VAL}"
        echo ""
        
        # Load Windsor environment to ensure variables are available
        if command -v windsor > /dev/null 2>&1; then
          eval "$(windsor env)" 2>/dev/null || true
        fi
        
        # Export environment variables for use in subsequent tasks
        export INCUS_REMOTE_NAME="${TEST_REMOTE_NAME}"
        export VM_INSTANCE_NAME="${VM_NAME}"
        export VM_IMAGE="${VM_IMAGE}"
        export VM_MEMORY="${VM_MEMORY}"
        export VM_CPU="${VM_CPU}"
        export VM_DISK_SIZE="${VM_DISK_SIZE}"
        if [ -n "${VM_NETWORK_NAME}" ]; then
          export VM_NETWORK_NAME="${VM_NETWORK_NAME}"
        fi
        export VM_STORAGE_POOL="${VM_STORAGE_POOL}"
        export VM_AUTOSTART="${VM_AUTOSTART}"
        export VM_INIT_WORKSPACE="${VM_INIT_WORKSPACE}"  # Set based on --no-workspace flag
        export DOCKER_HOST="${DOCKER_HOST_VAL}"
        
        # Test function
        test_step() {
          local test_name="$1"
          local test_command="$2"
          local expected_output="$3"
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Test: ${test_name}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          if eval "${test_command}" > /tmp/test_output.log 2>&1; then
            if [ -n "${expected_output}" ]; then
              if grep -q "${expected_output}" /tmp/test_output.log 2>/dev/null; then
                echo "✅ PASS: ${test_name}"
                TESTS_PASSED=$((TESTS_PASSED + 1))
                return 0
              else
                echo "❌ FAIL: ${test_name} - Expected output '${expected_output}' not found"
                echo ""
                echo "Command output:"
                cat /tmp/test_output.log
                echo ""
                echo "To debug:"
                echo "  - Run command manually: ${test_command}"
                echo "  - Check VM state: task vm:info -- ${VM_NAME}"
                TESTS_FAILED=$((TESTS_FAILED + 1))
                FAILED_TESTS+=("${test_name}")
                return 1
              fi
            else
              echo "✅ PASS: ${test_name}"
              TESTS_PASSED=$((TESTS_PASSED + 1))
              return 0
            fi
          else
            echo "❌ FAIL: ${test_name}"
            echo ""
            echo "Command output:"
            cat /tmp/test_output.log
            echo ""
            echo "To debug:"
            echo "  - Run command manually: ${test_command}"
            echo "  - Check VM state: task vm:info -- ${VM_NAME}"
            echo "  - SSH into VM: task vm:ssh -- ${VM_NAME}"
            TESTS_FAILED=$((TESTS_FAILED + 1))
            FAILED_TESTS+=("${test_name}")
            return 1
          fi
        }
        
        # Step 1: Verify Remote Connection
        test_step "Remote connection exists" \
          "incus remote list --format csv | grep -q '^${TEST_REMOTE_NAME},' || incus remote list | grep -q '${TEST_REMOTE_NAME}'" \
          ""
        
        test_step "Can connect to remote" \
          "incus list ${TEST_REMOTE_NAME}: --format csv > /dev/null" \
          ""
        
        # Step 2: Generate Terraform Variables
        # Environment variables are already set from Step 0
        test_step "Generate terraform.tfvars" \
          "task vm:generate-tfvars" \
          "Generated"
        
        test_step "terraform.tfvars file exists" \
          "test -f terraform/vm/terraform.tfvars" \
          ""
        
        # Step 3: Ensure VM image is available on remote
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Step 3: Ensure VM Image is Available"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        
        # Check if jq is available
        if ! command -v jq > /dev/null 2>&1; then
          echo "⚠️  Warning: 'jq' not found. Using fallback method to check images..."
          USE_JQ=false
        else
          USE_JQ=true
        fi
        
        # Set error handling
        set +e  # Temporarily disable exit on error for image checks
        
        # Check if VM image exists on remote (check by alias and type)
        IMAGE_EXISTS=false
        IMAGE_IS_VM=false
        
        if [ "${USE_JQ}" = "true" ]; then
          # Get all images with the alias and check their types using jq
          IMAGE_JSON=$(incus image list "${TEST_REMOTE_NAME}:" --format json 2>/dev/null || echo "[]")
          
          # Check each image to see if it has the alias and is a VM
          if echo "${IMAGE_JSON}" | grep -q "\"aliases\".*\"${VM_IMAGE}\""; then
            # Find the fingerprint of images with this alias
            for img_entry in $(echo "${IMAGE_JSON}" | jq -r '.[] | select(.aliases[]? | .name == "'"${VM_IMAGE}"'") | .fingerprint' 2>/dev/null); do
              if [ -n "${img_entry}" ] && [ "${img_entry}" != "null" ]; then
                # Check the type of this specific image
                IMG_TYPE=$(incus image info "${TEST_REMOTE_NAME}:${img_entry}" --format json 2>/dev/null | jq -r '.type' 2>/dev/null || echo "")
                if [ "${IMG_TYPE}" = "virtual-machine" ]; then
                  IMAGE_EXISTS=true
                  IMAGE_IS_VM=true
                  echo "✅ VM image '${VM_IMAGE}' already exists on remote '${TEST_REMOTE_NAME}' (fingerprint: ${img_entry})"
                  TESTS_PASSED=$((TESTS_PASSED + 1))
                  break
                elif [ "${IMG_TYPE}" = "container" ]; then
                  echo "⚠️  Image '${VM_IMAGE}' exists but is a container image (fingerprint: ${img_entry})"
                  echo "   Removing container image and will copy VM variant..."
                  # Remove the container image
                  incus image delete "${TEST_REMOTE_NAME}:${img_entry}" 2>/dev/null || true
                  # Remove the alias if it still exists
                  incus image alias delete "${TEST_REMOTE_NAME}:${VM_IMAGE}" 2>/dev/null || true
                fi
              fi
            done
          fi
        else
          # Fallback: use incus commands directly without jq
          # Check if image alias exists
          if incus image alias list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | grep -q "^${VM_IMAGE},"; then
            # Get the fingerprint for this alias
            IMG_FINGERPRINT=$(incus image alias list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | grep "^${VM_IMAGE}," | cut -d',' -f2 | head -1)
            if [ -n "${IMG_FINGERPRINT}" ]; then
              # Check image type by trying to get image info
              IMG_INFO=$(incus image info "${TEST_REMOTE_NAME}:${IMG_FINGERPRINT}" 2>/dev/null || echo "")
              if echo "${IMG_INFO}" | grep -q "Type:.*virtual-machine"; then
                IMAGE_EXISTS=true
                IMAGE_IS_VM=true
                echo "✅ VM image '${VM_IMAGE}' already exists on remote '${TEST_REMOTE_NAME}'"
                TESTS_PASSED=$((TESTS_PASSED + 1))
              elif echo "${IMG_INFO}" | grep -q "Type:.*container"; then
                echo "⚠️  Image '${VM_IMAGE}' exists but is a container image"
                echo "   Removing container image and will copy VM variant..."
                incus image delete "${TEST_REMOTE_NAME}:${IMG_FINGERPRINT}" 2>/dev/null || true
                incus image alias delete "${TEST_REMOTE_NAME}:${VM_IMAGE}" 2>/dev/null || true
              fi
            fi
          fi
        fi
        
        # Re-enable error handling
        set -e
        
        if [ "${IMAGE_EXISTS}" = "false" ] || [ "${IMAGE_IS_VM}" = "false" ]; then
          set +e  # Temporarily disable exit on error for image copy operations
          echo "Copying VM image '${VM_IMAGE}' from images remote..."
          
          if [ "${USE_JQ}" = "true" ]; then
            # Get images from the images remote and find the VM variant using jq
            IMAGES_REMOTE_JSON=$(incus image list "images:" --format json 2>/dev/null || echo "[]")
            
            # Find VM image fingerprint with the matching alias
            VM_FINGERPRINT=$(echo "${IMAGES_REMOTE_JSON}" | \
              jq -r '.[] | select(.type == "virtual-machine" and (.aliases[]? | .name == "'"${VM_IMAGE}"'")) | .fingerprint' 2>/dev/null | \
              head -1 || echo "")
            
            if [ -z "${VM_FINGERPRINT}" ] || [ "${VM_FINGERPRINT}" = "null" ]; then
              # Try alternative: look for the VM/24.04 or similar VM-specific aliases
              ALT_ALIAS="VM/24.04"
              VM_FINGERPRINT=$(echo "${IMAGES_REMOTE_JSON}" | \
                jq -r '.[] | select(.type == "virtual-machine" and (.aliases[]? | .name == "'"${ALT_ALIAS}"'")) | .fingerprint' 2>/dev/null | \
                head -1 || echo "")
              if [ -n "${VM_FINGERPRINT}" ] && [ "${VM_FINGERPRINT}" != "null" ]; then
                echo "   Using alternative VM image alias: ${ALT_ALIAS}"
              fi
            fi
          else
            # Fallback: use incus commands to find VM images
            echo "   Searching for VM images on images remote..."
            # List all images and filter for VM type and ubuntu
            VM_FINGERPRINT=""
            for img_line in $(incus image list "images:" --format csv 2>/dev/null | grep -i ubuntu || true); do
              img_fp=$(echo "${img_line}" | cut -d',' -f1)
              if [ -n "${img_fp}" ]; then
                img_info=$(incus image info "images:${img_fp}" 2>/dev/null || echo "")
                if echo "${img_info}" | grep -q "Type:.*virtual-machine"; then
                  # Check if it has the right alias
                  if incus image alias list "images:" --format csv 2>/dev/null | grep -q ",${img_fp},"; then
                    alias_name=$(incus image alias list "images:" --format csv 2>/dev/null | grep ",${img_fp}," | cut -d',' -f1 | head -1)
                    if [ "${alias_name}" = "${VM_IMAGE}" ] || [ "${alias_name}" = "VM/24.04" ]; then
                      VM_FINGERPRINT="${img_fp}"
                      break
                    fi
                  fi
                fi
              fi
            done
          fi
          
          if [ -n "${VM_FINGERPRINT}" ] && [ "${VM_FINGERPRINT}" != "null" ]; then
            # Copy by fingerprint to ensure we get the VM variant
            echo "   Found VM image fingerprint: ${VM_FINGERPRINT}"
            
            # Delete existing alias if it exists (might point to container image)
            if incus image alias list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | grep -q "^${VM_IMAGE},"; then
              echo "   Removing existing alias '${VM_IMAGE}' (may point to container image)..."
              incus image alias delete "${TEST_REMOTE_NAME}:${VM_IMAGE}" 2>/dev/null || true
            fi
            
            # Try to copy with alias first
            COPY_SUCCESS=false
            if incus image copy "images:${VM_FINGERPRINT}" "${TEST_REMOTE_NAME}:" --alias "${VM_IMAGE}" > /tmp/image_copy.log 2>&1; then
              COPY_SUCCESS=true
            else
              # If copy with alias failed, try copying without alias then adding alias
              echo "   Copy with alias failed, trying without alias..."
              if incus image copy "images:${VM_FINGERPRINT}" "${TEST_REMOTE_NAME}:" > /tmp/image_copy.log 2>&1; then
                # Add alias to the copied image
                COPIED_FP=$(incus image list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | grep "${VM_FINGERPRINT}" | cut -d',' -f1 | head -1)
                if [ -n "${COPIED_FP}" ]; then
                  if incus image alias create "${TEST_REMOTE_NAME}:${VM_IMAGE}" "${COPIED_FP}" > /tmp/alias_create.log 2>&1; then
                    COPY_SUCCESS=true
                  else
                    echo "   Warning: Could not create alias, but image was copied (fingerprint: ${COPIED_FP})"
                    COPY_SUCCESS=true  # Image is copied, alias is optional
                  fi
                fi
              fi
            fi
            
            if [ "${COPY_SUCCESS}" = "true" ]; then
              echo "✅ VM image '${VM_IMAGE}' copied successfully to remote '${TEST_REMOTE_NAME}'"
              TESTS_PASSED=$((TESTS_PASSED + 1))
              
              # Verify the copied image is a VM image
              VERIFICATION_SUCCESS=false
              
              if [ "${USE_JQ}" = "true" ]; then
                # Try to get fingerprint by alias
                COPIED_FINGERPRINT=$(incus image list "${TEST_REMOTE_NAME}:" --format json 2>/dev/null | \
                  jq -r '.[] | select(.aliases[]? | .name == "'"${VM_IMAGE}"'") | .fingerprint' 2>/dev/null | head -1)
                
                if [ -n "${COPIED_FINGERPRINT}" ] && [ "${COPIED_FINGERPRINT}" != "null" ]; then
                  COPIED_TYPE=$(incus image info "${TEST_REMOTE_NAME}:${COPIED_FINGERPRINT}" --format json 2>/dev/null | jq -r '.type' 2>/dev/null || echo "")
                  if [ "${COPIED_TYPE}" = "virtual-machine" ]; then
                    echo "✅ Verified: Copied image is a VM image"
                    VERIFICATION_SUCCESS=true
                  fi
                fi
                
                # If verification by alias failed, try by fingerprint directly
                if [ "${VERIFICATION_SUCCESS}" = "false" ]; then
                  COPIED_TYPE=$(incus image info "${TEST_REMOTE_NAME}:${VM_FINGERPRINT}" --format json 2>/dev/null | jq -r '.type' 2>/dev/null || echo "")
                  if [ "${COPIED_TYPE}" = "virtual-machine" ]; then
                    echo "✅ Verified: Copied image is a VM image (by fingerprint)"
                    VERIFICATION_SUCCESS=true
                  fi
                fi
              else
                # Fallback verification using incus commands
                COPIED_INFO=$(incus image info "${TEST_REMOTE_NAME}:${VM_IMAGE}" 2>/dev/null || echo "")
                if echo "${COPIED_INFO}" | grep -q "Type:.*virtual-machine"; then
                  echo "✅ Verified: Copied image is a VM image"
                  VERIFICATION_SUCCESS=true
                else
                  # Try by fingerprint
                  COPIED_INFO=$(incus image info "${TEST_REMOTE_NAME}:${VM_FINGERPRINT}" 2>/dev/null || echo "")
                  if echo "${COPIED_INFO}" | grep -q "Type:.*virtual-machine"; then
                    echo "✅ Verified: Copied image is a VM image (by fingerprint)"
                    VERIFICATION_SUCCESS=true
                  fi
                fi
              fi
              
              # If verification failed, check if we can at least confirm the image exists
              if [ "${VERIFICATION_SUCCESS}" = "false" ]; then
                # Check if image exists by alias (most reliable check)
                if incus image alias list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | grep -q "^${VM_IMAGE},"; then
                  echo "⚠️  Warning: Could not verify image type, but alias exists. Continuing..."
                  echo "   (Image alias: ${VM_IMAGE})"
                # Check if image exists by fingerprint
                elif incus image list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | grep -q "${VM_FINGERPRINT}"; then
                  echo "⚠️  Warning: Could not verify image type, but image exists. Continuing..."
                  echo "   (Image fingerprint: ${VM_FINGERPRINT})"
                # Check if any image with the alias exists
                elif incus image list "${TEST_REMOTE_NAME}:" --format json 2>/dev/null | grep -q "\"aliases\".*\"${VM_IMAGE}\""; then
                  echo "⚠️  Warning: Could not verify image type, but alias exists. Continuing..."
                  echo "   (Image alias: ${VM_IMAGE})"
                else
                  echo "❌ FAIL: Copied image verification failed - image may not exist"
                  echo "   Expected fingerprint: ${VM_FINGERPRINT}"
                  echo "   Expected alias: ${VM_IMAGE}"
                  echo ""
                  echo "   Available images on remote:"
                  incus image list "${TEST_REMOTE_NAME}:" --format csv 2>/dev/null | head -5 || echo "   (could not list images)"
                  TESTS_FAILED=$((TESTS_FAILED + 1))
                  FAILED_TESTS+=("Image availability")
                  exit 1
                fi
              fi
            else
              echo "❌ FAIL: Failed to copy VM image by fingerprint"
              cat /tmp/image_copy.log
              TESTS_FAILED=$((TESTS_FAILED + 1))
              FAILED_TESTS+=("Image availability")
              exit 1
            fi
          else
            echo "❌ FAIL: Could not find VM image '${VM_IMAGE}' on images remote"
            echo "   Searched for VM images with alias '${VM_IMAGE}' or 'VM/24.04'"
            if [ "${USE_JQ}" = "true" ]; then
              echo ""
              echo "   Available VM image aliases on images remote:"
              echo "${IMAGES_REMOTE_JSON}" | jq -r '.[] | select(.type == "virtual-machine") | .aliases[]? | .name' 2>/dev/null | grep -i ubuntu | head -10 || echo "   (could not list images)"
            else
              echo ""
              echo "   Try running: incus image list images: | grep -i ubuntu"
            fi
            TESTS_FAILED=$((TESTS_FAILED + 1))
            FAILED_TESTS+=("Image availability")
            echo ""
            echo "═══════════════════════════════════════════════════════════════"
            echo "Test Summary: ${TESTS_PASSED} passed, ${TESTS_FAILED} failed"
            echo "═══════════════════════════════════════════════════════════════"
            exit 1
          fi
        fi
        
        # Re-enable error handling after image operations
        set -e
        
        # Step 4: Initialize Terraform
        test_step "Terraform initialization" \
          "task vm:terraform:init" \
          ""
        
        # Step 5: Check if VM already exists (cleanup if needed)
        if incus list "${TEST_REMOTE_NAME}:${VM_NAME}" --format csv -c n 2>/dev/null | grep -q "^${VM_NAME}$"; then
          echo ""
          echo "⚠️  Warning: VM '${VM_NAME}' already exists. Cleaning up..."
          task vm:delete || true
          sleep 5
        fi
        
        # Step 6: Create VM (skip setup-env for faster testing, we'll test it separately)
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Creating VM (this may take a few minutes)..."
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        
        # Generate tfvars, init, and apply
        task vm:generate-tfvars > /dev/null 2>&1
        task vm:terraform:init > /dev/null 2>&1
        
        if task vm:terraform:apply > /tmp/terraform_apply.log 2>&1; then
          echo "✅ VM created successfully"
          TESTS_PASSED=$((TESTS_PASSED + 1))
        else
          echo "❌ FAIL: VM creation failed"
          cat /tmp/terraform_apply.log
          TESTS_FAILED=$((TESTS_FAILED + 1))
          FAILED_TESTS+=("VM creation")
          echo ""
          echo "═══════════════════════════════════════════════════════════════"
          echo "Test Summary: ${TESTS_PASSED} passed, ${TESTS_FAILED} failed"
          echo "═══════════════════════════════════════════════════════════════"
          exit 1
        fi
        
        # Wait for VM to be ready
        echo "Waiting for VM to be ready..."
        MAX_WAIT=120
        ELAPSED=0
        while [ ${ELAPSED} -lt ${MAX_WAIT} ]; do
          if incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- true 2>/dev/null; then
            echo "✅ VM agent is ready"
            break
          fi
          sleep 5
          ELAPSED=$((ELAPSED + 5))
          echo "  Waiting... (${ELAPSED}s/${MAX_WAIT}s)"
        done
        
        if [ ${ELAPSED} -ge ${MAX_WAIT} ]; then
          echo "❌ FAIL: VM agent not ready after ${MAX_WAIT} seconds"
          TESTS_FAILED=$((TESTS_FAILED + 1))
          FAILED_TESTS+=("VM agent ready")
        else
          TESTS_PASSED=$((TESTS_PASSED + 1))
        fi
        
        # Step 7: Verify VM exists and is running
        test_step "VM exists in list" \
          "incus list ${TEST_REMOTE_NAME}:${VM_NAME} --format csv -c n | grep -q '^${VM_NAME}$'" \
          ""
        
        test_step "VM is running" \
          "incus list ${TEST_REMOTE_NAME}:${VM_NAME} --format json | grep -q '\"status\":\"Running\"'" \
          ""
        
        # Step 8: Verify VM has IP address
        echo ""
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        echo "Test: VM has IP address"
        echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
        
        VM_IP=""
        set +e  # Temporarily disable exit on error for IP extraction
        
        # If using a physical network, try to get IP from that interface first
        if [ -n "${VM_NETWORK_NAME:-}" ] && [ "${VM_NETWORK_NAME}" != "incusbr0" ]; then
          # Physical network - get IP from all interfaces and filter for non-bridge IPs
          # Look for IPs that are not in common bridge/Docker ranges
          VM_IP=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- \
            ip -4 addr show 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | \
            grep -vE '^127\.|^172\.17\.|^10\.53\.68\.' | head -n 1 || echo "")
        fi
        
        # Fallback: try eth0 (for bridge networks)
        if [ -z "${VM_IP}" ]; then
          VM_IP=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- \
            ip -4 addr show eth0 2>/dev/null | grep -oE 'inet [0-9.]+' | awk '{print $2}' | head -n 1 2>/dev/null || echo "")
        fi
        
        # Fallback: try hostname -I and filter out bridge/Docker IPs
        if [ -z "${VM_IP}" ]; then
          VM_IP=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- \
            hostname -I 2>/dev/null | awk '{for(i=1;i<=NF;i++) if($i !~ /^127\.|^172\.17\.|^10\.53\.68\./) {print $i; exit}}' || echo "")
        fi
        
        # Fallback: get from incus list, but prefer IPs on physical network interface
        if [ -z "${VM_IP}" ]; then
          # Get all IPs from incus list
          ALL_IPS=$(incus list "${TEST_REMOTE_NAME}:${VM_NAME}" --format json 2>/dev/null | \
            grep -oE '"IPv4":"[0-9.]+' | cut -d'"' -f4 || echo "")
          
          # If using physical network, prefer IP not on bridge
          if [ -n "${VM_NETWORK_NAME:-}" ] && [ "${VM_NETWORK_NAME}" != "incusbr0" ]; then
            VM_IP=$(echo "${ALL_IPS}" | grep -vE '^172\.17\.|^10\.53\.68\.' | head -n 1 || echo "")
          fi
          
          # If still no IP, use first available
          if [ -z "${VM_IP}" ]; then
            VM_IP=$(echo "${ALL_IPS}" | head -n 1 || echo "")
          fi
        fi
        
        set -e  # Re-enable exit on error
        
        if [ -n "${VM_IP}" ] && echo "${VM_IP}" | grep -qE '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$'; then
          echo "✅ PASS: VM has IP address (${VM_IP})"
          TESTS_PASSED=$((TESTS_PASSED + 1))
        else
          echo "❌ FAIL: VM does not have a valid IP address"
          echo "   Attempted to get IP from: ip addr, hostname -I, and incus list"
          TESTS_FAILED=$((TESTS_FAILED + 1))
          FAILED_TESTS+=("VM IP address")
        fi
        
        # Step 9: Setup developer environment (skip if local)
        if [ "${TEST_REMOTE_NAME}" != "local" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Setting up developer environment..."
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "  This may take 5-10 minutes (installing packages, Docker, etc.)"
          echo "  Progress will be logged to /tmp/setup_env.log"
          
          # Run setup-env with timeout (15 minutes max)
          SETUP_TIMEOUT=900  # 15 minutes in seconds
          set +e  # Temporarily disable exit on error for timeout handling
          set +u  # Temporarily disable unset variable checking for background process handling
          
          # Initialize variables
          SETUP_SUCCESS=false
          EXIT_CODE=0
          
          # Check for timeout command (try both timeout and gtimeout for macOS Homebrew)
          TIMEOUT_CMD=""
          if command -v timeout > /dev/null 2>&1; then
            TIMEOUT_CMD="timeout"
          elif command -v gtimeout > /dev/null 2>&1; then
            TIMEOUT_CMD="gtimeout"
          fi
          
          if [ -n "${TIMEOUT_CMD}" ]; then
            # Use timeout command if available
            if ${TIMEOUT_CMD} ${SETUP_TIMEOUT} task vm:create:setup-env -- ${VM_NAME} > /tmp/setup_env.log 2>&1; then
              SETUP_SUCCESS=true
              EXIT_CODE=0
            else
              EXIT_CODE=$?
              SETUP_SUCCESS=false
            fi
          else
            # Fallback: Use background process with kill for timeout (works on macOS)
            echo "  Using background process timeout (${SETUP_TIMEOUT}s)..."
            task vm:create:setup-env -- ${VM_NAME} > /tmp/setup_env.log 2>&1 &
            SETUP_PID=$!
            
            # Wait a moment for the process to start and get a valid PID
            sleep 0.5
            
            # Verify we have a PID (but be lenient - the process might have already completed)
            if [ -z "${SETUP_PID:-}" ] || [ "${SETUP_PID:-0}" -eq 0 ] 2>/dev/null; then
              # No valid PID - check if the task already completed (might have failed fast)
              if [ -f /tmp/setup_env.log ] && [ -s /tmp/setup_env.log ]; then
                # Log exists and has content - check log for completion/interruption
                echo "⚠️  Warning: Could not get process PID, checking log for status..."
                # Check if task was interrupted or completed
                if grep -q "Signal received" /tmp/setup_env.log 2>/dev/null || \
                   grep -q "interrupt" /tmp/setup_env.log 2>/dev/null || \
                   grep -q "Forcing shutdown" /tmp/setup_env.log 2>/dev/null; then
                  echo "   Task was interrupted (found in log)"
                  SETUP_SUCCESS=false
                  EXIT_CODE=130  # SIGINT exit code
                else
                  # Wait a short time to see if process completes, then check log
                  sleep 2
                  # If log file is still being written to, the process is likely still running
                  # Since we can't manage it without a PID, treat as failure
                  echo "   Cannot determine task status without PID"
                  SETUP_SUCCESS=false
                  EXIT_CODE=1
                fi
              else
                echo "❌ FAIL: Failed to start setup task in background (no PID, no log)"
                SETUP_SUCCESS=false
                EXIT_CODE=1
              fi
            else
              # We have a valid PID - set up timeout and wait
              # Start a background process that will kill the task after timeout
              (
                set +u  # Disable unset variable checking in subshell
                TIMEOUT_VAL=${SETUP_TIMEOUT}
                PID_VAL=${SETUP_PID}
                sleep ${TIMEOUT_VAL}
                if [ -n "${PID_VAL:-}" ] && [ "${PID_VAL:-0}" -ne 0 ] && kill -0 ${PID_VAL} 2>/dev/null; then
                  echo "  ⚠️  Setup task timed out after $((TIMEOUT_VAL / 60)) minutes, killing process..."
                  kill -TERM ${PID_VAL} 2>/dev/null || true
                  sleep 2
                  kill -KILL ${PID_VAL} 2>/dev/null || true
                fi
              ) &
              KILLER_PID=$!
              
              # Wait for the setup task to complete
              if wait ${SETUP_PID} 2>/dev/null; then
                # Task completed successfully
                EXIT_CODE=$?
                # Kill the killer process
                kill ${KILLER_PID} 2>/dev/null || true
                wait ${KILLER_PID} 2>/dev/null || true
                SETUP_SUCCESS=true
              else
                # Task failed or was killed
                EXIT_CODE=$?
                # Kill the killer process
                kill ${KILLER_PID} 2>/dev/null || true
                wait ${KILLER_PID} 2>/dev/null || true
                # Check if it was killed due to timeout
                if [ ${EXIT_CODE} -eq 143 ] || [ ${EXIT_CODE} -eq 137 ]; then
                  # SIGTERM (143) or SIGKILL (137) - timeout
                  EXIT_CODE=124
                fi
                SETUP_SUCCESS=false
              fi
            fi
          fi
          
          if [ "${SETUP_SUCCESS}" = "true" ]; then
            echo "✅ Developer environment setup completed"
            TESTS_PASSED=$((TESTS_PASSED + 1))
          else
            echo ""
            echo "═══════════════════════════════════════════════════════════════"
            echo "❌ CRITICAL FAILURE: Developer Environment Setup Failed"
            echo "═══════════════════════════════════════════════════════════════"
            
            if [ -n "${EXIT_CODE:-}" ] && [ ${EXIT_CODE} -eq 124 ]; then
              echo ""
              echo "The setup task timed out after $((SETUP_TIMEOUT / 60)) minutes."
              echo "This usually means a package installation or service start is hanging."
            else
              echo ""
              echo "The setup task failed with exit code: ${EXIT_CODE}"
            fi
            
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Error Details (last 50 lines of log):"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            if [ -f /tmp/setup_env.log ]; then
              cat /tmp/setup_env.log | tail -50
            else
              echo "  No log file found at /tmp/setup_env.log"
            fi
            
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Debugging Steps:"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "1. Check the full setup log on the VM:"
            echo "   incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- cat /tmp/setup_env.log 2>/dev/null || echo 'Log not found on VM'"
            echo ""
            echo "2. Check if the user was created:"
            echo "   incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- id ${CURRENT_USER} 2>&1"
            echo ""
            echo "3. Check VM resources (memory, disk):"
            echo "   incus info ${TEST_REMOTE_NAME}:${VM_NAME} | grep -E '(Memory|Disk)'"
            echo ""
            echo "4. Try to manually run setup-env:"
            echo "   task vm:create:setup-env -- ${VM_NAME}"
            echo ""
            echo "5. Or SSH into the VM and debug interactively:"
            echo "   task vm:ssh -- ${VM_NAME}"
            echo "   # Then check: /tmp/setup_env.log, system logs, package installation status"
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Common Issues and Fixes:"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "• User creation failed:"
            echo "  - Check if user already exists: incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- id ${CURRENT_USER}"
            echo "  - Manually create user if needed (see debug-aqua.sh script)"
            echo ""
            echo "• Package installation hanging:"
            echo "  - Check VM has enough memory/disk: incus info ${TEST_REMOTE_NAME}:${VM_NAME}"
            echo "  - Check network connectivity: incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- ping -c 3 8.8.8.8"
            echo ""
            echo "• Docker installation failed:"
            echo "  - Check Docker service: incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- systemctl status docker"
            echo "  - Check disk space: incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- df -h"
            echo ""
            echo "• Homebrew/Aqua installation failed:"
            echo "  - Check network access: incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- curl -I https://github.com"
            echo "  - Check if Homebrew installed: incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- test -f /home/linuxbrew/.linuxbrew/bin/brew && echo 'Homebrew exists'"
            echo ""
            
            TESTS_FAILED=$((TESTS_FAILED + 1))
            FAILED_TESTS+=("Developer environment setup")
            
            echo ""
            echo "═══════════════════════════════════════════════════════════════"
            echo "⚠️  TEST STOPPED: Cannot continue without developer environment"
            echo "═══════════════════════════════════════════════════════════════"
            echo ""
            echo "The test is stopping here because the developer environment setup"
            echo "is required for subsequent tests (workspace initialization, tool"
            echo "verification, etc.)."
            echo ""
            echo "After fixing the issue, you can:"
            echo "  1. Re-run the full test: task vm:test -- ${TEST_REMOTE_NAME}"
            echo "  2. Or just re-run setup-env: task vm:create:setup-env -- ${VM_NAME}"
            echo ""
            echo "═══════════════════════════════════════════════════════════════"
            echo "Test Summary (so far): ${TESTS_PASSED} passed, ${TESTS_FAILED} failed"
            echo "═══════════════════════════════════════════════════════════════"
            
            set -e  # Re-enable exit on error
            set -u  # Re-enable unset variable checking
            exit 1  # Exit early - don't continue with tests that depend on setup-env
          fi
          
          set -e  # Re-enable exit on error
          set -u  # Re-enable unset variable checking
        fi
        
        # Step 9b: Initialize workspace (if enabled and remote)
        if [ "${TEST_REMOTE_NAME}" != "local" ] && [ "${VM_INIT_WORKSPACE}" = "true" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Step 9b: Initialize Workspace"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Get workspace name
          WORKSPACE_NAME=$(basename "${PROJECT_ROOT}")
          WORKSPACE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
          
          if task vm:init-workspace -- ${VM_NAME} > /tmp/init_workspace.log 2>&1; then
            echo "✅ Workspace initialization completed"
            TESTS_PASSED=$((TESTS_PASSED + 1))
            
            # Verify workspace exists
            test_step "Workspace directory exists" \
              "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- test -d ${WORKSPACE_PATH}" \
              ""
          else
            echo "❌ FAIL: Workspace initialization failed"
            cat /tmp/init_workspace.log | tail -20
            TESTS_FAILED=$((TESTS_FAILED + 1))
            FAILED_TESTS+=("Workspace initialization")
          fi
        elif [ "${VM_INIT_WORKSPACE}" = "false" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Step 9b: Workspace Initialization (skipped)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "  Workspace initialization is disabled (--no-workspace flag or VM_INIT_WORKSPACE=false)"
        fi
        
        # Step 10: Verify developer tools are installed
        test_step "Git is installed" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- git --version" \
          "git version"
        
        test_step "Curl is installed" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- curl --version" \
          "curl"
        
        test_step "Docker is installed" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- docker --version 2>&1" \
          "Docker version"
        
        # Step 11: Verify user was created
        test_step "User account exists" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- id -u ${CURRENT_USER} > /dev/null 2>&1" \
          ""
        
        test_step "User matches host username" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- su - ${CURRENT_USER} -c 'whoami' 2>/dev/null | grep -q '^${CURRENT_USER}$'" \
          ""
        
        # Step 12: Verify Git configuration
        if [ "${TEST_REMOTE_NAME}" != "local" ]; then
          GIT_NAME=$(git config --global user.name 2>/dev/null || echo "")
          if [ -n "${GIT_NAME}" ]; then
            test_step "Git user.name is configured" \
              "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- su - ${CURRENT_USER} -c 'git config --global user.name' | grep -q '${GIT_NAME}'" \
              ""
          fi
          
          GIT_EMAIL=$(git config --global user.email 2>/dev/null || echo "")
          if [ -n "${GIT_EMAIL}" ]; then
            test_step "Git user.email is configured" \
              "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- su - ${CURRENT_USER} -c 'git config --global user.email' | grep -q '${GIT_EMAIL}'" \
              ""
          fi
        fi
        
        # Step 13: Verify SSH keys (if remote)
        if [ "${TEST_REMOTE_NAME}" != "local" ]; then
          test_step "SSH keys directory exists" \
            "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- test -d /home/${CURRENT_USER}/.ssh" \
            ""
          
          test_step "SSH authorized_keys exists" \
            "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- test -f /home/${CURRENT_USER}/.ssh/authorized_keys" \
            ""
        fi
        
        # Step 14: Verify Docker is running
        test_step "Docker service is running" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- systemctl is-active --quiet docker 2>/dev/null || incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- docker info > /dev/null 2>&1" \
          ""
        
        test_step "Docker is accessible" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- docker ps > /dev/null 2>&1 || incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- sudo docker ps > /dev/null 2>&1" \
          ""
        
        # Step 15: Verify SSH server is running
        test_step "SSH server is running" \
          "incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- systemctl is-active --quiet ssh 2>/dev/null || incus exec ${TEST_REMOTE_NAME}:${VM_NAME} -- systemctl is-active --quiet sshd 2>/dev/null" \
          ""
        
        # Step 16: Test SSH access (if we have IP and remote)
        if [ -n "${VM_IP}" ] && [ "${TEST_REMOTE_NAME}" != "local" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Test: SSH access to VM"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Test SSH connectivity (with timeout)
          if timeout 10 ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no ${CURRENT_USER}@${VM_IP} "echo 'SSH test successful'" > /tmp/ssh_test.log 2>&1; then
            echo "✅ PASS: SSH access to VM"
            TESTS_PASSED=$((TESTS_PASSED + 1))
          else
            echo "⚠️  WARNING: SSH access test failed (may be expected if keys not set up)"
            echo "   This is not a critical failure if other tests pass"
            cat /tmp/ssh_test.log | tail -5
          fi
        fi
        
        # Step 17: Test GitHub SSH connection (if remote and keys are set up)
        if [ "${TEST_REMOTE_NAME}" != "local" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Test: GitHub SSH connection"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          if incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- su - ${CURRENT_USER} -c "ssh -T -o StrictHostKeyChecking=no git@github.com 2>&1" | grep -q "successfully authenticated"; then
            echo "✅ PASS: GitHub SSH connection"
            TESTS_PASSED=$((TESTS_PASSED + 1))
          else
            echo "⚠️  WARNING: GitHub SSH connection test failed (may be expected if keys not set up)"
            echo "   This is not a critical failure if other tests pass"
          fi
        fi
        
        # Step 18: Windsor Setup and Up (if --windsor-up flag is set and workspace was initialized)
        if [ "${RUN_WINDSOR_UP}" = "true" ] && [ "${TEST_REMOTE_NAME}" != "local" ] && [ "${VM_INIT_WORKSPACE}" = "true" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Step 18: Windsor Setup and Up"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          WORKSPACE_NAME=$(basename "${PROJECT_ROOT}")
          WORKSPACE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
          
          # Ensure workspace exists
          if ! incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- test -d "${WORKSPACE_PATH}" 2>/dev/null; then
            echo "❌ FAIL: Workspace directory does not exist: ${WORKSPACE_PATH}"
            echo "   Windsor setup requires workspace to be initialized"
            TESTS_FAILED=$((TESTS_FAILED + 1))
            FAILED_TESTS+=("Windsor setup (workspace missing)")
          else
            echo "  Workspace directory: ${WORKSPACE_PATH}"
            
            # Step 18a: Install aqua packages
            echo ""
            echo "  Installing aqua packages..."
            set +e  # Temporarily disable exit on error
            AQUA_OUTPUT=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- su - ${CURRENT_USER} -c "
              cd ${WORKSPACE_PATH}
              source ~/.bashrc 2>/dev/null || true
              aqua install 2>&1
            " 2>&1)
            AQUA_EXIT=$?
            set -e  # Re-enable exit on error
            
            if [ ${AQUA_EXIT} -eq 0 ]; then
              echo "✅ PASS: aqua install completed"
              TESTS_PASSED=$((TESTS_PASSED + 1))
            else
              echo "❌ FAIL: aqua install failed"
              echo "${AQUA_OUTPUT}" | tail -20
              TESTS_FAILED=$((TESTS_FAILED + 1))
              FAILED_TESTS+=("Windsor setup (aqua install)")
            fi
            
            # Step 18b: Windsor init local
            echo ""
            echo "  Initializing Windsor context 'local'..."
            set +e  # Temporarily disable exit on error
            WINDSOR_INIT_OUTPUT=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- su - ${CURRENT_USER} -c "
              cd ${WORKSPACE_PATH}
              source ~/.bashrc 2>/dev/null || true
              windsor init local 2>&1
            " 2>&1)
            WINDSOR_INIT_EXIT=$?
            set -e  # Re-enable exit on error
            
            if [ ${WINDSOR_INIT_EXIT} -eq 0 ]; then
              echo "✅ PASS: windsor init local completed"
              TESTS_PASSED=$((TESTS_PASSED + 1))
            else
              echo "❌ FAIL: windsor init local failed"
              echo "${WINDSOR_INIT_OUTPUT}" | tail -20
              TESTS_FAILED=$((TESTS_FAILED + 1))
              FAILED_TESTS+=("Windsor setup (init)")
            fi
            
            # Step 18c: Windsor up
            echo ""
            echo "  Running windsor up (this may take several minutes)..."
            set +e  # Temporarily disable exit on error
            WINDSOR_UP_OUTPUT=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- su - ${CURRENT_USER} -c "
              cd ${WORKSPACE_PATH}
              source ~/.bashrc 2>/dev/null || true
              windsor up 2>&1
            " 2>&1)
            WINDSOR_UP_EXIT=$?
            set -e  # Re-enable exit on error
            
            if [ ${WINDSOR_UP_EXIT} -eq 0 ]; then
              echo "✅ PASS: windsor up completed"
              TESTS_PASSED=$((TESTS_PASSED + 1))
            else
              echo "❌ FAIL: windsor up failed"
              echo "${WINDSOR_UP_OUTPUT}" | tail -30
              TESTS_FAILED=$((TESTS_FAILED + 1))
              FAILED_TESTS+=("Windsor setup (windsor up)")
            fi
            
            # Step 18d: Verify pods are running
            if [ ${WINDSOR_UP_EXIT} -eq 0 ]; then
              echo ""
              echo "  Verifying pod status..."
              set +e  # Temporarily disable exit on error
              
              # Wait a moment for pods to stabilize
              sleep 5
              
              # Get pod status
              # Try to find kubectl (may be installed via aqua)
              POD_STATUS_OUTPUT=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- su - ${CURRENT_USER} -c "
                cd ${WORKSPACE_PATH}
                source ~/.bashrc 2>/dev/null || true
                # Set KUBECONFIG if it exists
                if [ -f ${WORKSPACE_PATH}/contexts/local/.kube/config ]; then
                  export KUBECONFIG=${WORKSPACE_PATH}/contexts/local/.kube/config
                fi
                # Try kubectl (may be in PATH via aqua)
                kubectl get pods --all-namespaces 2>&1 || echo 'kubectl not available or kubeconfig not found'
              " 2>&1)
              POD_STATUS_EXIT=$?
              
              if [ ${POD_STATUS_EXIT} -eq 0 ]; then
                echo "✅ PASS: kubectl access working"
                echo ""
                echo "  Pod Status:"
                echo "${POD_STATUS_OUTPUT}" | head -30
                
                # Count running pods
                RUNNING_PODS=$(echo "${POD_STATUS_OUTPUT}" | grep -c "Running" || echo "0")
                PENDING_PODS=$(echo "${POD_STATUS_OUTPUT}" | grep -c "Pending" || echo "0")
                FAILED_PODS=$(echo "${POD_STATUS_OUTPUT}" | grep -cE "(Error|CrashLoopBackOff|ImagePullBackOff)" || echo "0")
                
                echo ""
                echo "  Pod Summary:"
                echo "    Running: ${RUNNING_PODS}"
                echo "    Pending: ${PENDING_PODS}"
                echo "    Failed:  ${FAILED_PODS}"
                
                if [ ${FAILED_PODS} -gt 0 ]; then
                  echo ""
                  echo "  ⚠️  Warning: Some pods are in failed state:"
                  echo "${POD_STATUS_OUTPUT}" | grep -E "(Error|CrashLoopBackOff|ImagePullBackOff)" | head -10
                fi
                
                TESTS_PASSED=$((TESTS_PASSED + 1))
              else
                echo "⚠️  WARNING: Could not verify pod status (kubectl may not be configured yet)"
                echo "${POD_STATUS_OUTPUT}" | tail -10
              fi
              
              set -e  # Re-enable exit on error
            fi
          fi
        elif [ "${RUN_WINDSOR_UP}" = "true" ] && [ "${VM_INIT_WORKSPACE}" != "true" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Step 18: Windsor Setup (skipped)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "  Windsor setup requires workspace initialization"
          echo "  Workspace initialization was skipped (--no-workspace flag)"
        fi
        
        # Final Summary
        echo ""
        echo "═══════════════════════════════════════════════════════════════"
        echo "Test Summary"
        echo "═══════════════════════════════════════════════════════════════"
        echo "Tests Passed: ${TESTS_PASSED}"
        echo "Tests Failed: ${TESTS_FAILED}"
        echo ""
        
        if [ ${TESTS_FAILED} -gt 0 ]; then
          echo "Failed Tests:"
          for failed_test in "${FAILED_TESTS[@]}"; do
            echo "  - ${failed_test}"
          done
          echo ""
        fi
        
        # Display VM Information (if all tests passed)
        if [ ${TESTS_FAILED} -eq 0 ]; then
          echo ""
          echo "═══════════════════════════════════════════════════════════════"
          echo "VM Information"
          echo "═══════════════════════════════════════════════════════════════"
          echo ""
          
          # Extract and display key information
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Basic Information"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Remote: ${TEST_REMOTE_NAME}"
          echo "VM Name: ${VM_NAME}"
          echo "User: ${CURRENT_USER}"
          echo "Image: ${VM_IMAGE}"
          
          # Get status using JSON if available, otherwise use list output
          if command -v jq > /dev/null 2>&1; then
            VM_STATUS=$(incus list "${TEST_REMOTE_NAME}:${VM_NAME}" --format json 2>/dev/null | \
              jq -r '.[0].status // "unknown"' 2>/dev/null || echo "unknown")
          else
            VM_STATUS=$(incus list "${TEST_REMOTE_NAME}:${VM_NAME}" --format csv -c ns 2>/dev/null | \
              grep "^${VM_NAME}," | cut -d',' -f2 || echo "unknown")
          fi
          echo "Status: ${VM_STATUS}"
          
          # IP Address
          if [ -n "${VM_IP:-}" ]; then
            echo "IP Address: ${VM_IP}"
          else
            # Try to get IP again
            VM_IP=$(incus list "${TEST_REMOTE_NAME}:${VM_NAME}" --format json 2>/dev/null | \
              grep -o '"4":{[^}]*"addresses":[^}]*}' | \
              grep -o '"address":"[^"]*"' | head -1 | cut -d'"' -f4 || echo "")
            if [ -n "${VM_IP}" ]; then
              echo "IP Address: ${VM_IP}"
            else
              echo "IP Address: (not available)"
            fi
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Resource Allocation"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Memory
          if [ -n "${VM_MEMORY:-}" ]; then
            echo "Memory (allocated): ${VM_MEMORY}"
          fi
          # Get memory usage from inside the VM
          MEMORY_USAGE=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- free -h 2>/dev/null | grep "^Mem:" || echo "")
          if [ -n "${MEMORY_USAGE}" ]; then
            echo "Memory usage: ${MEMORY_USAGE}"
          fi
          
          # CPU
          if [ -n "${VM_CPU:-}" ]; then
            echo "CPU (allocated): ${VM_CPU} cores"
          fi
          # Get CPU info from inside the VM
          CPU_COUNT=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- nproc 2>/dev/null || echo "")
          if [ -n "${CPU_COUNT}" ]; then
            echo "CPU (available): ${CPU_COUNT} cores"
          fi
          
          # Disk
          if [ -n "${VM_DISK_SIZE:-}" ]; then
            echo "Disk (allocated): ${VM_DISK_SIZE}"
          fi
          # Get disk usage from inside the VM
          DISK_INFO=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- df -h / 2>/dev/null | tail -1 || echo "")
          if [ -n "${DISK_INFO}" ]; then
            echo "Disk usage (root filesystem):"
            echo "  ${DISK_INFO}"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Network Configuration"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          if [ -n "${VM_NETWORK_NAME:-}" ]; then
            echo "Network Interface: ${VM_NETWORK_NAME}"
          else
            echo "Network Interface: (default Incus network)"
          fi
          
          # Get network interfaces from inside the VM
          NETWORK_INTERFACES=$(incus exec "${TEST_REMOTE_NAME}:${VM_NAME}" -- ip -4 addr show 2>/dev/null | \
            grep -E "^[0-9]+:|inet " | head -10 || echo "")
          if [ -n "${NETWORK_INTERFACES}" ]; then
            echo "Network Interfaces:"
            echo "${NETWORK_INTERFACES}" | while IFS= read -r line; do
              echo "  ${line}"
            done
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Access Information"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          if [ -n "${VM_IP:-}" ]; then
            echo "SSH: ssh ${CURRENT_USER}@${VM_IP}"
            echo "     Or: task vm:ssh -- ${VM_NAME}"
          else
            echo "SSH: task vm:ssh -- ${VM_NAME}"
          fi
          echo "Shell: task vm:shell -- ${VM_NAME}"
          echo "Info:  task vm:info -- ${VM_NAME}"
          
          # Workspace path if initialized
          if [ "${VM_INIT_WORKSPACE}" = "true" ] && [ "${TEST_REMOTE_NAME}" != "local" ]; then
            WORKSPACE_NAME=$(basename "${PROJECT_ROOT}")
            WORKSPACE_PATH="/home/${CURRENT_USER}/${WORKSPACE_NAME}"
            echo "Workspace: ${WORKSPACE_PATH}"
          fi
          
          echo ""
          echo "═══════════════════════════════════════════════════════════════"
        fi
        
        # Cleanup: Delete the test VM (unless --keep flag was set)
        if [ "${SKIP_CLEANUP}" = "true" ]; then
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Skipping cleanup (--keep flag set)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "VM '${VM_NAME}' on remote '${TEST_REMOTE_NAME}' has been left running."
          echo "To delete it later, run:"
          echo "  windsor context set test"
          echo "  task vm:delete"
          echo ""
          echo "VM IP address: ${VM_IP:-unknown}"
        else
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Cleaning up test VM..."
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Cleanup: Environment variables are still set from Step 0
          if task vm:delete > /tmp/cleanup.log 2>&1; then
            echo "✅ Test VM deleted successfully"
          else
            echo "⚠️  Warning: Failed to delete test VM. Manual cleanup may be required."
            echo "   Run: windsor context set test && task vm:delete"
            cat /tmp/cleanup.log | tail -10
          fi
        fi
        
        echo ""
        echo "═══════════════════════════════════════════════════════════════"
        
        if [ ${TESTS_FAILED} -eq 0 ]; then
          echo "✅ ALL TESTS PASSED"
          echo "═══════════════════════════════════════════════════════════════"
          exit 0
        else
          echo "❌ SOME TESTS FAILED"
          echo "═══════════════════════════════════════════════════════════════"
          exit 1
        fi

  #-----------------------------------------------------------------------------------------------------------------------
  # Runner Tasks (GitHub Actions Runner Setup)
  #-----------------------------------------------------------------------------------------------------------------------

  runner:initialize:
    desc: Initializes a new Incus VM for GitHub Actions runner
    cmds:
      - task: runner:install-aqua
      - task: runner:install-docker
      - task: runner:create-runner-user
      - task: runner:setup-ssh
      - task: runner:install-windsor-cli
      - |
        echo "Runner VM initialization complete!"
        echo "Next steps:"
        echo "  1. Install GitHub Actions runner: task vm:runner:install-github-runner -- <vm-name>"
        echo "  2. Or follow the manual installation steps in the runbook"
    silent: true

  runner:install-aqua:
    desc: Installs aqua package manager on the runner VM
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:install-aqua -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        
        echo "Installing aqua on ${REMOTE}:${VM_NAME}..."
        
        # Install aqua using the official installer
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Check if aqua is already installed
          if command -v aqua >/dev/null 2>&1; then
            echo 'aqua is already installed'
            aqua version
            exit 0
          fi
          
          # Download and install aqua
          curl -fsSL https://raw.githubusercontent.com/aquaproj/aqua-installer/main/aqua-installer | bash
          
          # Add aqua to PATH for current session
          export PATH=\"\${HOME}/.local/share/aquaproj-aqua/bin:\${PATH}\"
          
          # Verify installation
          aqua version
          
          echo 'aqua installed successfully'
        "
        
        echo "aqua installation complete"
    silent: true

  runner:install-docker:
    desc: Installs Docker on the runner VM
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:install-docker -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        
        echo "Installing Docker on ${REMOTE}:${VM_NAME}..."
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Check if Docker is already installed
          if command -v docker >/dev/null 2>&1; then
            echo 'Docker is already installed'
            docker --version
            exit 0
          fi
          
          # Update package index
          apt-get update -y
          
          # Install prerequisites
          apt-get install -y \
            ca-certificates \
            curl \
            gnupg \
            lsb-release
          
          # Add Docker's official GPG key
          install -m 0755 -d /etc/apt/keyrings
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
          chmod a+r /etc/apt/keyrings/docker.gpg
          
          # Set up Docker repository
          echo \\
            \"deb [arch=\$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\
            \$(. /etc/os-release && echo \"\$VERSION_CODENAME\") stable\" | \\
            tee /etc/apt/sources.list.d/docker.list > /dev/null
          
          # Install Docker Engine
          apt-get update -y
          apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
          
          # Start and enable Docker service
          systemctl start docker
          systemctl enable docker
          
          # Verify installation
          docker --version
          docker ps
          
          echo 'Docker installed successfully'
        "
        
        echo "Docker installation complete"
    silent: true
    
  runner:create-runner-user:
    desc: Creates a dedicated runner user for GitHub Actions
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:create-runner-user -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        RUNNER_USER="{{.RUNNER_USER}}"
        
        echo "Creating runner user '${RUNNER_USER}' on ${REMOTE}:${VM_NAME}..."
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Check if user already exists
          if id \"${RUNNER_USER}\" &>/dev/null; then
            echo \"User ${RUNNER_USER} already exists\"
            exit 0
          fi
          
          # Create runner user with home directory
          useradd -m -s /bin/bash \"${RUNNER_USER}\"
          
          # Add runner user to docker group (if Docker is installed)
          if getent group docker >/dev/null 2>&1; then
            usermod -aG docker \"${RUNNER_USER}\"
            echo \"Added ${RUNNER_USER} to docker group\"
          fi
          
          # Add runner user to sudo group (for administrative tasks)
          usermod -aG sudo \"${RUNNER_USER}\"
          
          # Set up sudoers to allow passwordless sudo (for automation)
          echo \"${RUNNER_USER} ALL=(ALL) NOPASSWD:ALL\" | tee /etc/sudoers.d/${RUNNER_USER}
          chmod 0440 /etc/sudoers.d/${RUNNER_USER}
          
          echo \"User ${RUNNER_USER} created successfully\"
        "
        
        echo "Runner user creation complete"
    silent: true
    
  runner:setup-ssh:
    desc: Sets up SSH access for the runner user
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:setup-ssh -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        RUNNER_USER="{{.RUNNER_USER}}"
        
        echo "Setting up SSH access for ${RUNNER_USER} on ${REMOTE}:${VM_NAME}..."
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Install OpenSSH server if not already installed
          if ! command -v sshd >/dev/null 2>&1; then
            apt-get update -y
            apt-get install -y openssh-server
          fi
          
          # Ensure SSH service is running
          systemctl start ssh || systemctl start sshd || true
          systemctl enable ssh || systemctl enable sshd || true
          
          # Create .ssh directory for runner user
          mkdir -p /home/${RUNNER_USER}/.ssh
          chmod 700 /home/${RUNNER_USER}/.ssh
          chown ${RUNNER_USER}:${RUNNER_USER} /home/${RUNNER_USER}/.ssh
          
          echo 'SSH setup complete'
          echo 'To add your SSH key, run:'
          echo '  incus exec ${REMOTE}:${VM_NAME} -- bash -c \"echo \\\"<your-public-key>\\\" >> /home/${RUNNER_USER}/.ssh/authorized_keys\"'
          echo '  incus exec ${REMOTE}:${VM_NAME} -- bash -c \"chmod 600 /home/${RUNNER_USER}/.ssh/authorized_keys\"'
          echo '  incus exec ${REMOTE}:${VM_NAME} -- bash -c \"chown ${RUNNER_USER}:${RUNNER_USER} /home/${RUNNER_USER}/.ssh/authorized_keys\"'
        "
        
        echo "SSH setup complete"
    silent: true
    
  runner:install-github-runner:
    desc: Installs and configures GitHub Actions runner
    cmds:
      - |
        set -euo pipefail
        
        # Validate required environment variables
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:install-github-runner -- <vm-name>"
          exit 1
        fi
        
        if [ -z "{{.GITHUB_RUNNER_REPO_URL}}" ]; then
          echo "Error: GITHUB_RUNNER_REPO_URL environment variable is not defined"
          echo "Set it in your windsor.yaml file (e.g., https://github.com/user/repo)"
          exit 1
        fi
        
        if [ -z "{{.GITHUB_RUNNER_TOKEN}}" ]; then
          echo "Error: GITHUB_RUNNER_TOKEN environment variable is not defined"
          echo "Set it in your windsor.yaml file (get the token from GitHub Settings → Actions → Runners)"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        RUNNER_USER="{{.RUNNER_USER}}"
        RUNNER_HOME="{{.RUNNER_HOME}}"
        REPO_URL="{{.GITHUB_RUNNER_REPO_URL}}"
        TOKEN="{{.GITHUB_RUNNER_TOKEN}}"
        RUNNER_VERSION="{{.GITHUB_RUNNER_VERSION | default ""}}"
        RUNNER_ARCH="{{.GITHUB_RUNNER_ARCH | default "x64"}}"
        
        # Get runner version if not specified
        if [ -z "${RUNNER_VERSION}" ]; then
          echo "Determining latest runner version..."
          RUNNER_VERSION=$(curl -s https://api.github.com/repos/actions/runner/releases/latest | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/' | sed 's/v//')
          
          if [ -z "${RUNNER_VERSION}" ]; then
            echo "Error: Could not determine latest runner version"
            exit 1
          fi
        fi
        
        echo "Installing GitHub Actions runner on ${REMOTE}:${VM_NAME}..."
        echo "Repository: ${REPO_URL}"
        echo "Runner version: ${RUNNER_VERSION}"
        echo "Architecture: ${RUNNER_ARCH}"
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Create actions-runner directory as runner user
          sudo -u ${RUNNER_USER} mkdir -p ${RUNNER_HOME}/actions-runner
          cd ${RUNNER_HOME}/actions-runner
          
          # Download runner as runner user
          sudo -u ${RUNNER_USER} curl -o actions-runner-linux-${RUNNER_ARCH}-${RUNNER_VERSION}.tar.gz -L \\
            https://github.com/actions/runner/releases/download/v${RUNNER_VERSION}/actions-runner-linux-${RUNNER_ARCH}-${RUNNER_VERSION}.tar.gz
          
          # Extract as runner user
          sudo -u ${RUNNER_USER} tar xzf ./actions-runner-linux-${RUNNER_ARCH}-*.tar.gz
          
          # Configure runner as runner user
          sudo -u ${RUNNER_USER} ./config.sh --url ${REPO_URL} --token ${TOKEN} --unattended
          
          # Install as systemd service (runs as runner user)
          sudo ./svc.sh install ${RUNNER_USER}
          sudo ./svc.sh start
          
          echo 'GitHub Actions runner installed and started'
        "
        
        echo "GitHub Actions runner installation complete"
    silent: true
    
  runner:install-packages:
    desc: Installs additional packages commonly needed for GitHub Actions runners
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:install-packages -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        
        echo "Installing additional packages on ${REMOTE}:${VM_NAME}..."
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          apt-get update -y
          apt-get install -y \\
            build-essential \\
            git \\
            curl \\
            wget \\
            python3 \\
            python3-pip \\
            nodejs \\
            npm \\
            default-jre \\
            default-jdk \\
            unzip \\
            software-properties-common \\
            jq \\
            yq \\
            rsync \\
            openssh-client
          
          echo 'Additional packages installed successfully'
        "
        
        echo "Package installation complete"
    silent: true
    
  runner:install-windsor-cli:
    desc: Installs Windsor CLI on the runner VM
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:install-windsor-cli -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        WINDSOR_VERSION="{{.WINDSOR_CLI_VERSION | default "v0.8.1"}}"
        
        echo "Installing Windsor CLI ${WINDSOR_VERSION} on ${REMOTE}:${VM_NAME}..."
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Check if Windsor CLI is already installed
          if command -v windsor >/dev/null 2>&1; then
            INSTALLED_VERSION=\$(windsor version 2>/dev/null | head -n1 || echo '')
            echo \"Windsor CLI already installed: \${INSTALLED_VERSION}\"
            exit 0
          fi
          
          # Detect platform and architecture
          PLATFORM=\$(uname -s | tr '[:upper:]' '[:lower:]')
          ARCH=\$(uname -m)
          
          # Map architecture
          if [ \"\${ARCH}\" = \"x86_64\" ]; then
            ARCH=\"amd64\"
          elif [ \"\${ARCH}\" = \"arm64\" ] || [ \"\${ARCH}\" = \"aarch64\" ]; then
            ARCH=\"arm64\"
          fi
          
          # Install to /usr/local/bin for system-wide access
          INSTALL_DIR=\"/usr/local/bin\"
          
          # Download and install Windsor CLI directly (pipe curl to tar, no intermediate file)
          DOWNLOAD_URL=\"https://github.com/windsorcli/cli/releases/download/\${WINDSOR_VERSION}/windsor_\${WINDSOR_VERSION#v}_\${PLATFORM}_\${ARCH}.tar.gz\"
          echo \"Downloading and installing Windsor CLI from: \${DOWNLOAD_URL}\"
          
          curl -fsSL \"\${DOWNLOAD_URL}\" | sudo tar -xz -C \${INSTALL_DIR} windsor
          sudo chmod +x \${INSTALL_DIR}/windsor
          
          # Verify installation
          \${INSTALL_DIR}/windsor version
          echo \"Windsor CLI installed successfully to \${INSTALL_DIR}\"
        "
        
        echo "Windsor CLI installation complete"
    silent: true
    
  runner:clean-work-dir:
    desc: Cleans the actions-runner/_work/test/test directory on the runner VM
    cmds:
      - |
        set -euo pipefail
        
        if [ -z "{{.CLI_ARGS}}" ]; then
          echo "Error: VM name is required"
          echo "Usage: task vm:runner:clean-work-dir -- <vm-name>"
          exit 1
        fi
        
        VM_NAME="{{.CLI_ARGS}}"
        REMOTE="{{.INCUS_REMOTE_NAME}}"
        RUNNER_USER="{{.RUNNER_USER}}"
        RUNNER_HOME="{{.RUNNER_HOME}}"
        WORK_DIR="${RUNNER_HOME}/actions-runner/_work/test/test"
        
        echo "Cleaning work directory on ${REMOTE}:${VM_NAME}..."
        echo "Target directory: ${WORK_DIR}"
        
        incus exec "${REMOTE}:${VM_NAME}" -- bash -c "
          set -euo pipefail
          
          # Check if directory exists
          if [ ! -d \"${WORK_DIR}\" ]; then
            echo \"Directory ${WORK_DIR} does not exist, nothing to clean\"
            exit 0
          fi
          
          # Switch to runner user and remove all contents (preserves hidden files/dirs like .windsor)
          sudo -u ${RUNNER_USER} bash -c \"
            cd ${WORK_DIR}
            sudo rm -rf .windsor/.docker-cache
            sudo rm -rf .windsor
            sudo rm -rf .volumes
            rm -rf *
            echo 'Cleaned ${WORK_DIR} (hidden files/directories like .windsor are preserved)'
          \"
          
          echo 'Work directory cleaned successfully'
        "
        
        echo "Work directory cleanup complete"
    silent: true

  help:
    silent: true
    desc: Ubuntu VM commands
    cmds:
      - |
        echo "Ubuntu Virtual Machine (Incus)"
        echo ""
        echo "Create:"
        echo "    task vm:create                  # Create VM using Terraform"
        echo "    task vm:generate-tfvars         # Generate terraform.tfvars from environment variables"
        echo "    task vm:terraform:init           # Initialize Terraform"
        echo "    task vm:terraform:plan           # Show Terraform plan"
        echo "    task vm:terraform:apply          # Apply Terraform configuration"
        echo ""
        echo "Manage:"
        echo "    task vm:start [-- <instance-name>]"
        echo "    task vm:stop [-- <instance-name>]"
        echo "    task vm:restart [-- <instance-name>]"
        echo "    task vm:list"
        echo "    task vm:info [-- <instance-name>]     # Get instance info"
        echo "    task vm:delete                  # Delete VM using Terraform"
        echo ""
        echo "Testing:"
        echo "    task vm:test -- <remote-name> [--keep] [--no-workspace] [--windsor-up]  # Test complete setup and validate VM"
        echo "      Options:"
        echo "        --keep, --no-cleanup    Keep VM running after test (default: delete VM)"
        echo "        --no-workspace          Skip workspace initialization (default: initialize workspace)"
        echo "        --windsor-up            Run windsor init and windsor up after workspace setup"
        echo ""
        echo "Access:"
        echo "    task vm:shell [-- <instance-name>]     # Interactive shell"
        echo "    task vm:ssh [-- <instance-name>]      # SSH into VM"
        echo "    task vm:ssh-info [-- <instance-name>] # Show SSH connection info"
        echo "    task vm:exec -- <command>               # Execute command"
        echo "    task vm:exec -- <instance-name> -- <command>"
        echo ""
        echo "Utilities:"
        echo "    task vm:init-workspace [-- <instance-name>]  # Initialize workspace in existing VM"
        echo "    task vm:copy-workspace [-- <instance-name>]  # Copy workspace (replaces existing)"
        echo "    task vm:add-workspace [-- <instance-name>]  # Add workspace (merges with existing)"
        echo "    task vm:sync-workspace [-- <instance-name>]  # Sync only changed files (rsync)"
        echo ""
        echo "GitHub Actions Runner Setup:"
        echo "    task vm:runner:initialize -- <vm-name>  # Initialize runner VM (installs all dependencies)"
        echo "    task vm:runner:install-aqua -- <vm-name>"
        echo "    task vm:runner:install-docker -- <vm-name>"
        echo "    task vm:runner:create-runner-user -- <vm-name>"
        echo "    task vm:runner:setup-ssh -- <vm-name>"
        echo "    task vm:runner:install-windsor-cli -- <vm-name>"
        echo "    task vm:runner:install-packages -- <vm-name>"
        echo "    task vm:runner:install-github-runner -- <vm-name>  # Install and configure GitHub Actions runner"
        echo "    task vm:runner:clean-work-dir -- <vm-name>"
        echo ""

